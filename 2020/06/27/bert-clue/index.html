<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="2.Transformers for Sentiment Analysis"><meta name="keywords" content=""><meta name="author" content="养生少女不熬叶"><meta name="copyright" content="养生少女不熬叶"><title>2.Transformers for Sentiment Analysis | 养生少女不熬叶</title><link rel="shortcut icon" href="https://s1.ax1x.com/2020/04/30/JLI6fA.png"><link rel="stylesheet" href="/yyyeuing.github.io/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/yyyeuing.github.io/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#数据处理"><span class="toc-number">1.</span> <span class="toc-text">数据处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网络模型"><span class="toc-number">2.</span> <span class="toc-text">网络模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型训练"><span class="toc-number">3.</span> <span class="toc-text">模型训练</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://s2.ax1x.com/2020/02/13/1qtzCQ.jpg"></div><div class="author-info__name text-center">养生少女不熬叶</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/yyyeuing.github.io/archives"><span class="pull-left">Articles</span><span class="pull-right">3</span></a><a class="author-info-articles__tags article-meta" href="/yyyeuing.github.io/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s1.ax1x.com/2020/04/30/JL5kCR.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/yyyeuing.github.io/">养生少女不熬叶</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">2.Transformers for Sentiment Analysis</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-06-27</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>基于bert实现的文本分类</p>
<a id="more"></a>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><ul>
<li>数据集使用TNEWS<a href="https://storage.googleapis.com/cluebenchmark/tasks/tnews_public.zip" target="_blank" rel="noopener">下载地址</a></li>
<li>数据集包括：训练集(53,360)，验证集(10,000)，测试集(10,000)</li>
<li>每条数据包括三个属性：分类的ID，分类的名称，新闻字符串。其中共包含15个标签。<blockquote>
<p>eg:<br>  {“label”: “108”, “label_desc”: “news_edu”, “sentence”: “上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？”, “keywords”: “”}</p>
</blockquote>
</li>
</ul>
<ol>
<li><p>定义InputExample类，其中包括：guid，text_a,text_b,label着四个属性。由于是单行文本分类的问题，text_b用不到，定义为None。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#  读取json文件</span><br><span class="line">class InputExample(object):</span><br><span class="line">    def __init__(self,guid,text_a,text_b&#x3D;None,lable &#x3D; None):</span><br><span class="line">        self.guid &#x3D; guid</span><br><span class="line">        self.text_a &#x3D; text_a</span><br><span class="line">        self.text_b &#x3D; text_b</span><br><span class="line">        self.label &#x3D; lable</span><br></pre></td></tr></table></figure>
</li>
<li><p>按行读取数据，每行读取到的[sentence]部分即为分类的语句，将其转换成一个一个的token后，再将token转换成对应的id。最后返回的是一个列表examples，里面包含每条处理后的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#  读取json文件</span><br><span class="line">def read_json(input_file):</span><br><span class="line">    with open(input_file, &#39;r&#39;, encoding&#x3D;&#39;UTF-8&#39;) as f:</span><br><span class="line">        reader &#x3D; f.readlines()</span><br><span class="line">        lines &#x3D; []</span><br><span class="line">        for line in reader:</span><br><span class="line">            lines.append(json.loads(line.strip()))</span><br><span class="line">        # print(lines)</span><br><span class="line">        return lines</span><br><span class="line"></span><br><span class="line">def get_examples(lines,set_type):</span><br><span class="line">    examples &#x3D; []</span><br><span class="line">    for (i, line) in enumerate (lines):</span><br><span class="line">        guid &#x3D; &quot;%s-%s&quot; % (set_type, i)</span><br><span class="line">        text_a &#x3D; tokenizer.convert_tokens_to_ids(tokenizer.tokenize(lines[i][&#39;sentence&#39;]))</span><br><span class="line">        text_b &#x3D; None</span><br><span class="line">        label &#x3D; str(line[&#39;label&#39;]) if set_type !&#x3D; &#39;test&#39; else &quot;100&quot;</span><br><span class="line">        examples.append(InputExample(guid &#x3D; guid,text_a&#x3D;text_a,text_b&#x3D;text_b, lable&#x3D; label))</span><br><span class="line">    # print(len(examples))</span><br><span class="line">    return examples</span><br><span class="line"></span><br><span class="line">def get_any_examples(data_dir):</span><br><span class="line">    return get_examples(read_json(data_dir), os.path.splitext(data_dir)[0])</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h2><p>使用bert预训练模型，具体可参考<a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">huggingface官方文档</a>   </p>
<ul>
<li>. bert模型类的输入主要分以下四个部分：<blockquote>
<ul>
<li>input_ids:文本对应的id序列,shape=[batch_size, sequence_length] </li>
<li>attention_mask:文本对应的pad标记序列,1表示该位置未被填充，0表示被填充。shape=[batch_size, sequence_length],  </li>
<li>token_type_ids:用于区分文本中两个句子的标记序列,0表示相应token属于第一个句子，11表示相应token属于第二个句子。shape=[batch_size, sequence_length]  </li>
</ul>
</blockquote>
</li>
</ul>
<p><img src="https://s1.ax1x.com/2020/06/27/NcnhdS.png" alt="蔡老板通俗的解释"><br>代码实现：<br>在bert中，限定输入的长度不能超过512，故定义max_length=512。不及512的地方补0，超过的进行裁剪。<br>定义四个列表：id_list、attention_masks_list、token_type_ids_list、labels_list，将每条数据转换得来的输入存入到相应的列表，最后将列表存入data字典中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def preprocess(data_dir,</span><br><span class="line">               max_length &#x3D; 512):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    处理数据</span><br><span class="line">    :param raw_data_fn: 原始数据文件名</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    examples &#x3D; get_any_examples(data_dir)</span><br><span class="line"></span><br><span class="line">    id_list &#x3D; []</span><br><span class="line">    attention_masks_list &#x3D; []</span><br><span class="line">    token_type_ids_list &#x3D; []</span><br><span class="line">    labels_list &#x3D; []</span><br><span class="line"></span><br><span class="line">    for example in examples:</span><br><span class="line"></span><br><span class="line">        input_ids &#x3D; example.text_a</span><br><span class="line"></span><br><span class="line">        label_list &#x3D; get_lables()</span><br><span class="line">        label_map &#x3D; &#123;label: i for i ,label in enumerate(label_list)&#125;</span><br><span class="line">        label  &#x3D; label_map[example.label]</span><br><span class="line"></span><br><span class="line">        if len(input_ids) &gt; max_length:</span><br><span class="line">            token &#x3D; tokenizer.tokenize(input_ids)</span><br><span class="line">            input_ids &#x3D; token[:max_length-2]</span><br><span class="line"></span><br><span class="line">        input_len &#x3D; len(input_ids)</span><br><span class="line">        padding_length &#x3D; max_length-input_len</span><br><span class="line"></span><br><span class="line">        attention_masks &#x3D; [1] * input_len + [0] * padding_length</span><br><span class="line">        input_ids &#x3D; input_ids + ([0] * padding_length)</span><br><span class="line">        token_type_ids &#x3D;[0] * max_length</span><br><span class="line"></span><br><span class="line">        id_list.append(input_ids)</span><br><span class="line">        attention_masks_list.append(attention_masks)</span><br><span class="line">        token_type_ids_list.append(token_type_ids)</span><br><span class="line">        labels_list.append(label)</span><br><span class="line"></span><br><span class="line">    data &#x3D; &#123;&#39;input_ids&#39;:id_list,</span><br><span class="line">            &#39;attention_masks&#39;:attention_masks_list,</span><br><span class="line">            &#39;token_type_ids&#39;:token_type_ids_list,</span><br><span class="line">            &#39;input_labels&#39;:labels_list&#125;</span><br><span class="line">    return data</span><br></pre></td></tr></table></figure>

<ul>
<li>bert模型调用<br>该任务用到的是transformers提供的BertForSequenceClassification<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 加载模型及配置方法</span><br><span class="line">   bert_config &#x3D; BertConfig.from_pretrained(model_name, num_labels&#x3D;15)  # 头条文本分类数据集为15类</span><br><span class="line">   model &#x3D; BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path&#x3D;model_name, config&#x3D;bert_config, cache_dir&#x3D;cache_dir)</span><br><span class="line">   optimizer &#x3D; torch.optim.Adam(model.parameters(), lr&#x3D;lr)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>该任务是单行文本问题，只需要输入input_ids和labels即可。output[0]为loss。(查看BertForSequenceClassification类函数可更清楚的了解模型的输入和输出)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def train(model, optimizer, data_loader):</span><br><span class="line">    min_loss &#x3D; float(&#39;inf&#39;)</span><br><span class="line">    for e in range(epochs):</span><br><span class="line">        for step,batch in enumerate(data_loader):</span><br><span class="line">            model.zero_grad()</span><br><span class="line">            model.train()</span><br><span class="line">            inputs &#x3D; &#123;&#39;input_ids&#39;:batch[0],</span><br><span class="line">                      &#39;attention_mask&#39;:batch[1],</span><br><span class="line">                      &#39;token_tpye_ids&#39;:batch[2],</span><br><span class="line">                      &#39;labels&#39;:batch[3]</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            output &#x3D; model(input_ids &#x3D; inputs[&#39;input_ids&#39;], labels &#x3D; inputs[&#39;labels&#39;])</span><br><span class="line">            # print(output[0])</span><br><span class="line"></span><br><span class="line">            train_loss &#x3D; output[0]</span><br><span class="line">            train_loss &#x3D; train_loss &#x2F; accumulation_steps</span><br><span class="line">            train_loss.backward()</span><br><span class="line"></span><br><span class="line">            # 每八次更新一下网络中的参数</span><br><span class="line">            if (step+1) % accumulation_steps &#x3D;&#x3D; 0:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            if (step+1) % accumulation_steps &#x3D;&#x3D; 1:</span><br><span class="line">                print(&#39;Train Epoch: &#123;&#125; [&#123;&#125;&#x2F;&#123;&#125;]  ||  train_loss: &#123;:.6f&#125;&#39;.format(</span><br><span class="line">                    e+1, step * batch_size, len(data_loader.dataset), train_loss.item()</span><br><span class="line">                ))</span><br><span class="line"></span><br><span class="line">        print(&#39;Train Epoch: &#123;&#125; || train_loss:&#123;:.6f&#125;.&#39;.format(e+1,train_loss.item()))</span><br><span class="line">        if train_loss &lt; min_loss:</span><br><span class="line">            min_loss &#x3D; train_loss</span><br><span class="line">            torch.save(model.state_dict(), path)</span><br></pre></td></tr></table></figure>

<p>附：</p>
<p><a href="https://bigganbing.github.io/2019/10/12/Learning-Bert/?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">bert超详细讲解播客(๑•̀ㅂ•́)و✧</a><br><a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">CLUE benchmark代码</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">养生少女不熬叶</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://yyyeuing.top/2020/06/27/bert-clue/">https://yyyeuing.top/2020/06/27/bert-clue/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="next-post pull-right"><a href="/yyyeuing.github.io/2020/04/30/TextClassifier/"><span>1-Simple Sentiment Analysis</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://s1.ax1x.com/2020/04/30/JL5kCR.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By 养生少女不熬叶</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/yyyeuing.github.io/js/utils.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/fancybox.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/sidebar.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/copy.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/fireworks.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/transition.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/scroll.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/head.js?version=1.7.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>