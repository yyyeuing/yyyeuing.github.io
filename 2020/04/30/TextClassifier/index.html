<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="1-Simple Sentiment Analysis"><meta name="keywords" content="Sentiment Analysis,nlp"><meta name="author" content="养生少女不熬叶"><meta name="copyright" content="养生少女不熬叶"><title>1-Simple Sentiment Analysis | 养生少女不熬叶</title><link rel="shortcut icon" href="https://s1.ax1x.com/2020/04/30/JLI6fA.png"><link rel="stylesheet" href="/yyyeuing.github.io/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/yyyeuing.github.io/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Simple-Sentiment-Analysis"><span class="toc-number">1.</span> <span class="toc-text">Simple Sentiment Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据处理："><span class="toc-number">1.1.</span> <span class="toc-text">数据处理：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#定义并训练模型"><span class="toc-number">1.2.</span> <span class="toc-text">定义并训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#附录"><span class="toc-number">2.</span> <span class="toc-text">附录</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://s2.ax1x.com/2020/02/13/1qtzCQ.jpg"></div><div class="author-info__name text-center">养生少女不熬叶</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/yyyeuing.github.io/archives"><span class="pull-left">Articles</span><span class="pull-right">2</span></a><a class="author-info-articles__tags article-meta" href="/yyyeuing.github.io/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://s1.ax1x.com/2020/04/30/JL5kCR.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/yyyeuing.github.io/">养生少女不熬叶</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">1-Simple Sentiment Analysis</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-30</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>电影评论情感分类最简版<br><a id="more"></a></p>
<h2 id="Simple-Sentiment-Analysis"><a href="#Simple-Sentiment-Analysis" class="headerlink" title="Simple Sentiment Analysis"></a>Simple Sentiment Analysis</h2><h3 id="数据处理："><a href="#数据处理：" class="headerlink" title="数据处理："></a>数据处理：</h3><ul>
<li>下载数据集，切分成训练集，验证集，测试集。</li>
<li>建立词汇表。由于词汇过多会导致训练时间过长，需要进行简化，可以通过选取最常出现的n个词汇，或者舍弃出现次数少于m次的词汇。每次输入是一批句子，句子太短的用pad进行填充，其次用unk来填充被舍弃的词汇。</li>
<li>创建迭代器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchtext import data</span><br><span class="line">import spacy</span><br><span class="line"></span><br><span class="line">#随机种子繁衍</span><br><span class="line">SEED = 1234</span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = True</span><br><span class="line">TEXT = data.Field(tokenize=&apos;spacy&apos;,tokenizer_language=&apos;en_core_web_sm&apos;)</span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br><span class="line"></span><br><span class="line">#下载数据集切分成训练集和测试集</span><br><span class="line">from torchtext import datasets</span><br><span class="line">train_data,test_data = datasets.IMDB.splits(TEXT,LABEL)</span><br><span class="line"></span><br><span class="line">print(f&apos;Number of training examples: &#123;len(train_data)&#125;&apos;)</span><br><span class="line">print(f&apos;Number of testing examples: &#123;len(test_data)&#125;&apos;)</span><br><span class="line">#print(vars(train_data.examples[0]))</span><br><span class="line"></span><br><span class="line">#将训练集切分为训练集和验证集</span><br><span class="line">import random</span><br><span class="line">train_data,valid_data = train_data.split(random_state = random.seed(SEED))</span><br><span class="line">print(f&apos;Number of training examples: &#123;len(train_data)&#125;&apos;)</span><br><span class="line">print(f&apos;Number of validation examples:&#123;len(valid_data)&#125;&apos;)</span><br><span class="line">print(f&apos;Number of testing examples: &#123;len(test_data)&#125;&apos;)</span><br><span class="line"></span><br><span class="line">#建立词汇表</span><br><span class="line">MAX_VOCAB_SIZE = 25000</span><br><span class="line">TEXT.build_vocab(train_data,max_size = MAX_VOCAB_SIZE)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br><span class="line">#输出“Unique tokens in TEXT vocabulary: 25002”是由于添加了&lt;unk&gt;和&lt;pad&gt;</span><br><span class="line">#&lt;unk&gt;是补充丢弃的词汇</span><br><span class="line">#&lt;pad&gt;是由于每次是输入一批句子，句子的长度要求一样，短句子会被填充</span><br><span class="line">print(f&apos;Unique tokens in TEXT vocabulary: &#123;len(TEXT.vocab)&#125;&apos;)</span><br><span class="line">print(f&apos;Unique tokens in TEXT vocabulary: &#123;len(LABEL.vocab)&#125;&apos;)</span><br><span class="line">print(f&apos;The most common words in the vocabulary and their frequencies:&#123;TEXT.vocab.freqs.most_common(20)&#125;&apos;)</span><br><span class="line">print(TEXT.vocab.itos[:10])# stoi (string to int) , itos (int to string)</span><br><span class="line">print(LABEL.vocab.stoi) #0:negative,1:positive</span><br><span class="line">#</span><br><span class="line">##迭代</span><br><span class="line">BATCH_SIZE = 64</span><br><span class="line">device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)</span><br><span class="line">train_iterator,valid_iterator,test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data,valid_data,test_data),</span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device = device</span><br><span class="line">)#返回一批示例，其中每个示例的长度都相似，从而最小化每个示例的填充量。</span><br></pre></td></tr></table></figure>
<h3 id="定义并训练模型"><a href="#定义并训练模型" class="headerlink" title="定义并训练模型"></a>定义并训练模型</h3><p>Three layers:embeddiong layer,RNN,linear layers</p>
<ol>
<li>Embedding layer:transform our sparse one-hot vector into a dense embedding vector.(a single fully connected layer)</li>
<li>RNN:take in our dense vector and the previous hidden state ht-1(to calculate the next hidden state ht)<br>RNN(循环神经网络)：一个序列当前的输出与前面的输出有关<br><img src="https://s1.ax1x.com/2020/05/01/JXovK1.png" alt><br><img src="https://s1.ax1x.com/2020/05/01/JXoL8J.png" alt></li>
<li>Linear layer:take the final hidden state and feed it through a fully connected layer,transforming it to the correct output dimension.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">#搭建模型</span><br><span class="line">import torch.nn as nn</span><br><span class="line">class RNN(nn.Module):</span><br><span class="line">    def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(input_dim,embedding_dim)</span><br><span class="line">        self.rnn = nn.RNN(embedding_dim,hidden_dim)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim,output_dim)</span><br><span class="line"></span><br><span class="line">    def forward(self,text):</span><br><span class="line">        #text = [sent len,batch size]</span><br><span class="line">        #embedded = [sent len,batch size,emb dim]</span><br><span class="line">        #output = [sent len, batch size,hid dim]</span><br><span class="line">        #hidden - [1,batch size,hid dim]</span><br><span class="line">        embedded = self.embedding(text)</span><br><span class="line">        output,hidden = self.rnn(embedded)</span><br><span class="line">        assert torch.equal(output[-1,:,:],hidden.squeeze(0))</span><br><span class="line">        return self.fc(hidden.squeeze(0))</span><br><span class="line"></span><br><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = 100</span><br><span class="line">HIDDEN_DIM = 256</span><br><span class="line">OUTPUT_DIM = 1</span><br><span class="line">model = RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM).to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义函数求可训练的参数数量</span><br><span class="line">def count_parameters(model):</span><br><span class="line">    return sum(p.numel() for p in model.parameters() if p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(f&apos;The model has &#123;count_parameters(model):,&#125; trainable paremeters&apos;)</span><br><span class="line"></span><br><span class="line">#训练模型</span><br><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr=1e-3)#优化器，更新模块参数；随机梯度下降法，SGD</span><br><span class="line">criterion = nn.BCEWithLogitsLoss().to(device)#损失函数，BCEWithLogitsLoss同时执行sigmoid和二元交叉熵步骤。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def binary_accuracy(preds,y):</span><br><span class="line">    #将预测结果四舍五入到最近结果</span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float()#转换为浮点数除法</span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    return acc</span><br><span class="line"></span><br><span class="line">def train(model,iterator,optimizer,criterion):</span><br><span class="line">    epoch_loss = 0</span><br><span class="line">    epoch_acc = 0</span><br><span class="line">    model.train()#让dropout和BN生效，此处没用到</span><br><span class="line"></span><br><span class="line">    for batch in iterator:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        predictions = model(batch.text).squeeze(1)</span><br><span class="line">        loss = criterion(predictions,batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions,batch.label)</span><br><span class="line">        loss.backward() #计算每个参数的梯度</span><br><span class="line">        optimizer.step()</span><br><span class="line">        epoch_loss += loss.item() #.item()从一个只包含一个值的张量中提取标量。</span><br><span class="line">        epoch_acc += acc.item()</span><br><span class="line">    return epoch_loss/len(iterator),epoch_acc/len(iterator)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def evaluate(model,iterator,criterion):</span><br><span class="line">    epoch_loss = 0</span><br><span class="line">    epoch_ass = 0</span><br><span class="line">    model.eval()</span><br><span class="line">    with torch.no_grad(): #不计算梯度</span><br><span class="line">        for batch in iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(1)</span><br><span class="line">            loss = criterion(predictions,batch.label)</span><br><span class="line">            acc = criterion(predictions,batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_ass += acc.item()</span><br><span class="line">    return epoch_loss/len(iterator),epoch_ass/len(iterator)</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">def epoch_time(start_time,end_time):</span><br><span class="line">    elapsed_time = end_time-start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time/60)</span><br><span class="line">    elapsed_secs = int(elapsed_time-(elapsed_mins*60))</span><br><span class="line">    return elapsed_mins,elapsed_secs</span><br><span class="line"></span><br><span class="line">N_EPOCHS = 5</span><br><span class="line">best_valid_loss = float(&apos;inf&apos;)</span><br><span class="line">for epoch in range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss,train_acc = train(model,train_iterator,optimizer,criterion)</span><br><span class="line">    valid_loss,valid_acc = evaluate(model,valid_iterator,criterion)</span><br><span class="line">    end_time = time.time()</span><br><span class="line">    epoch_mins,epoch_secs = epoch_time(start_time,end_time)</span><br><span class="line"></span><br><span class="line">    if valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(),&apos;tut1-model.pt&apos;)</span><br><span class="line"></span><br><span class="line">    print(f&apos;Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;&apos;)</span><br><span class="line">    print(f&apos;\tTrain Loss: &#123;train_loss:.3f&#125; | Train ACC: &#123;train_acc * 100 :.2f&#125;%&apos;)</span><br><span class="line">    print(f&apos;\tVal Loss: &#123;valid_loss:.3f&#125; | Val ACC: &#123;valid_acc * 100 :.2f&#125;%&apos;)</span><br><span class="line"></span><br><span class="line">model.load_state_dict(torch.load(&apos;tut1-model.pt&apos;))</span><br><span class="line">test_loss,test_acc = evaluate(model,test_iterator,criterion)</span><br><span class="line">print(f&apos;Test Loss: &#123;test_loss:.3f&#125; | Test Acc: &#123;test_acc *100 :.2f&#125;%&apos;)</span><br></pre></td></tr></table></figure>
<p>结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: 25000</span><br><span class="line">Number of testing examples: 25000</span><br><span class="line"></span><br><span class="line">Number of training examples: 17500</span><br><span class="line">Number of validation examples:7500</span><br><span class="line">Number of testing examples: 25000</span><br><span class="line"></span><br><span class="line">Unique tokens in TEXT vocabulary: 25002</span><br><span class="line">Unique tokens in TEXT vocabulary: 2</span><br><span class="line"></span><br><span class="line">The most common words in the vocabulary and their frequencies:[(&apos;the&apos;, 203566), (&apos;,&apos;, 192495), (&apos;.&apos;, 165618), (&apos;and&apos;, 109442), (&apos;a&apos;, 109116), (&apos;of&apos;, 100702), (&apos;to&apos;, 93766), (&apos;is&apos;, 76328), (&apos;in&apos;, 61255), (&apos;I&apos;, 54004), (&apos;it&apos;, 53508), (&apos;that&apos;, 49187), (&apos;&quot;&apos;, 44282), (&quot;&apos;s&quot;, 43329), (&apos;this&apos;, 42445), (&apos;-&apos;, 36692), (&apos;/&gt;&lt;br&apos;, 35752), (&apos;was&apos;, 35034), (&apos;as&apos;, 30384), (&apos;with&apos;, 29774)]</span><br><span class="line"></span><br><span class="line">[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;and&apos;, &apos;a&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]</span><br><span class="line"></span><br><span class="line">defaultdict(None, &#123;&apos;neg&apos;: 0, &apos;pos&apos;: 1&#125;)</span><br><span class="line"></span><br><span class="line">The model has 2,592,105 trainable paremeters</span><br><span class="line">Epoch: 01 | Epoch Time: 0m 52</span><br><span class="line">	Train Loss: 0.694 | Train ACC: 50.28%</span><br><span class="line">	Val Loss: 0.697 | Val ACC: 69.69%</span><br><span class="line">Epoch: 02 | Epoch Time: 0m 54</span><br><span class="line">	Train Loss: 0.693 | Train ACC: 49.87%</span><br><span class="line">	Val Loss: 0.697 | Val ACC: 69.70%</span><br><span class="line">Epoch: 03 | Epoch Time: 0m 53</span><br><span class="line">	Train Loss: 0.693 | Train ACC: 50.18%</span><br><span class="line">	Val Loss: 0.697 | Val ACC: 69.70%</span><br><span class="line">Epoch: 04 | Epoch Time: 0m 53</span><br><span class="line">	Train Loss: 0.693 | Train ACC: 49.77%</span><br><span class="line">	Val Loss: 0.697 | Val ACC: 69.69%</span><br><span class="line">Epoch: 05 | Epoch Time: 0m 53</span><br><span class="line">	Train Loss: 0.693 | Train ACC: 50.11%</span><br><span class="line">	Val Loss: 0.697 | Val ACC: 69.71%</span><br><span class="line">Test Loss: 0.709 | Test Acc: 70.92%</span><br></pre></td></tr></table></figure></p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>遇到的一些坑：anaconda中安装spacy后，出现<br>Warning: no model found for ‘en’<br>网上有很多教程可以解决，通常可以用以下代码解决<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure></p>
<p>但是也会存在一直下载不下来的情况，这时可以自己从官网上下载。最重要的一点就是下载时要看好版本，网上有很多教程推荐的版本已经很老的，如果下载的还是那个“老版本”，可能和我一样会出现以下报错<br>re.error: bad escape \p at position 257<br>所以下载时一定要看好自己spacy的版本，然后再去下载对应的en_core_web_sm版本。最后输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install en_core_web_sm-2.2.5.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>文章参考 <a href="https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb" target="_blank" rel="noopener">1 - Simple Sentiment Analysis.ipynb</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">养生少女不熬叶</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://yyyeuing.top/2020/04/30/TextClassifier/">https://yyyeuing.top/2020/04/30/TextClassifier/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/yyyeuing.github.io/tags/Sentiment-Analysis/">Sentiment Analysis</a><a class="post-meta__tags" href="/yyyeuing.github.io/tags/nlp/">nlp</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/yyyeuing.github.io/2020/02/14/cartoon/"><span>那些年一起看过的动漫</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://s1.ax1x.com/2020/04/30/JL5kCR.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By 养生少女不熬叶</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/yyyeuing.github.io/js/utils.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/fancybox.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/sidebar.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/copy.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/fireworks.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/transition.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/scroll.js?version=1.7.0"></script><script src="/yyyeuing.github.io/js/head.js?version=1.7.0"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>