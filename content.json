{"meta":{"title":"yyyeuing's blog","subtitle":null,"description":null,"author":"yyyeuing","url":"https://yyyeuing.top","root":"/yyyeuing.github.io/"},"pages":[{"title":"tags","date":"2019-05-23T11:45:17.000Z","updated":"2019-05-23T11:46:57.851Z","comments":false,"path":"tags/index.html","permalink":"https://yyyeuing.top/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-05-23T11:45:52.000Z","updated":"2019-05-23T11:47:23.598Z","comments":false,"path":"categories/index.html","permalink":"https://yyyeuing.top/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Attention-深度学习中的注意力机制","slug":"machine-learning","date":"2019-08-13T05:57:21.000Z","updated":"2019-08-13T14:49:26.535Z","comments":true,"path":"2019/08/13/machine-learning/","link":"","permalink":"https://yyyeuing.top/2019/08/13/machine-learning/","excerpt":"","text":"注意力模型（Attention Model）近年来被广泛的应用到NLP，图像识别，语音识别等不同程度的深度学习中。 Encoder-Decoder框架对于句子&lt;Source,Target&gt;,我们的目标是给定输入句子Source,期待通过Encoder-Decoder框架来生成目标句子Target.Source和Target可以是同一种语言，也可以是不同的语言。Source和Target分别由各自的单词序列构成。 Encoder就是对输入的句子进行编码，将输入句子通过非线性变换转换为中间语义表示C。 Decoder就是根据句子的中间语义表示C和之前已经生成的历史信息y1,y2……yi来生成i时刻要生成的单词yi;缺点：在生成目标句子的单词是，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的。这说明Source中任意单词对生成某个目标单词yi来说影响力都是相同的。相当于人眼看到画面眼中却没有焦点。举个栗子：如果将“我爱中国”这四个字翻译成英文，即“I love China”。用Encoder-Decoder框架逐步生成英文单词：“I”,“love”，“China”。在翻译“China”这个英文单词的时候，分心模型（Encoder-Decoder）里面每个中文单词对于翻译目标单词“China”贡献相同。 Attention模型Soft Attention模型针对上述栗子，如果引入Attention模型的话，在翻译“China”的时候，会体现中文单词对于翻译当前英文单词不同的影响程度，类似于给出一个概率分布值：（我：0.3）（爱：0.2）（中国：0.5）每个中文词组的概率代表了翻译当前单词“China”时，注意力分配模型分配给不同英文单词的注意力大小。Attention模型的关键在于：由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。生成目标句子单词的过程变成下面形式：那么生成目标句子某个单词，比如“China”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？？？为了方便解释，我们再次将上述的分心模型拿来，不过这次我们用RNN模型来代替它的Encoder和Decoder。那么下图👇就可以解释了Attention机制的本质思想我们将Attention机制从上面的Encoder-Decoder框架中剥离。将Source中的构成元素想象成是由一系列&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和key用来计算对应Value的权重系数。可表达为以下公式👇Attention机制的具体计算过程 阶段一：根据Query和Key计算两者的相似性或者相关性； 阶段二：对第一阶段的原始分值进行归一化处理 阶段三：根据权重系数对Value进行加权求和具体表现参考下图其中，第一阶段常见的方法有三种：求两者间的向量点积，求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值。第二阶段引入SoftMax。一方面可进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。第二阶段计算结果ai就是Value对应的权重系数，然后进行加权求和就可以得到Attention的数值。 Multi-Head AttentionGoogle提出的新概念，是Attention机制的完善。通过把Query,Key,Value通过参数矩阵映射一下，然后再做Attention，把这个过程重复做h次，最后将结果拼接起来。 Self Attention模型Attention机制发生在Target的元素Query和Source中所有元素之间。而Self Attention指的是Source内部元素之间或者Target内部元素之间发生的Attention机制，可以理解为Target=Source这种特殊情况下的注意力机制。Self Attention可以捕获同一个句子中单词之间的一些句法特征或者语义特征，会更容易捕获句子中长距离的相互依赖的特征。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yyyeuing.top/categories/机器学习/"}],"tags":[]},{"title":"flag不倒","slug":"happy-August","date":"2019-08-08T06:53:21.000Z","updated":"2019-08-10T07:02:10.511Z","comments":true,"path":"2019/08/08/happy-August/","link":"","permalink":"https://yyyeuing.top/2019/08/08/happy-August/","excerpt":"","text":"整理文件的时候，看见了尘封已久的播客，想了想还是打开了自己的播客，想起自己曾经一个人学习搭建播客，遇到过无数的问题，想找人请教却是不知找谁，只能自己苦苦研究。当自己的播客终于可以正常运行的时候，自己又有了新的事情需要去做，就把播客扔在了一边，现在再次看到，不免有点小感慨。六月和七月是我最忙碌的两个月，一直在准备考试，害怕挂科的恐惧总是充斥着自己。因为我一直觉得自己并不是一个天资聪颖的人，相反，可能还是一个有点笨手笨脚的人。每天早起去自习，晚上很晚才回到寝室，或许自己在自习室的时间并没有全部用来学习，但这样这样的生活模式却让我觉得很安心，没有一种天天玩乐的负罪感。所幸最后成绩还算勉强。结束了六七月份的考试，七月底，我回家了。回家的那几天，没有任何的学习压力，每天都按时睡觉按时起床吃早饭，也没有和任何同学出去玩耍，只想自己一个人静静的待在家里闲躺着。我觉得这可能是一种长久压抑过后的自我逃避，提不起任何学习的兴趣，也不想和外界交流。父母亲人见到我都说我瘦了，我自己却是感觉不到，只是体重秤上日益减小的数字告诉我，我真的变瘦了。八月份，我返回了学校，见到了L先生。刚刚见到他的时候，我觉得是那么的熟悉但是又陌生，本想好了一见到他就要给他一个大大的拥抱，可是他过来牵我的手的时候，我却下意识的躲开了。在家待久了连见L先生都有些不知如何交流了。所幸L先生还是我的L先生，那种陌生的感觉并没有持续很久，我们还是和以前一样，像“蜜恋中的老夫老妻”。每天一起早起去自习，他送我去工学馆或者实验室，然后自己再去综合楼训练，一起吃饭，晚上一起回寝。恰逢七夕，一起出去庆祝，送给对方自己精心准备的礼物。陪伴是最长情的告白，一切都是那么的简单而又幸福。回到学校，压力又来了，实验室的学习自己前期本来就已经落下了很多，现在做project，读paper总觉得有点儿力不从心，自己只有花更多的时间或许才能赶上那么一点。不管怎么说，还是坚持吧。我的八月份，不希望再浑浑噩噩，很早以前就和L先生立下了flag：1.提前学习数据结构2.预习概率论再加上自己本来的打算3.deep-learning这都是很宽泛很宽泛的学习计划，但还是希望自己能达到心里预计的学习成果吧。","categories":[{"name":"杂文","slug":"杂文","permalink":"https://yyyeuing.top/categories/杂文/"}],"tags":[]}]}