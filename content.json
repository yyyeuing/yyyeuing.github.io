{"meta":{"title":"yyyeuing's blog","subtitle":null,"description":null,"author":"yyyeuing","url":"https://yyyeuing.top","root":"/yyyeuing.github.io/"},"pages":[{"title":"categories","date":"2019-05-23T11:45:52.000Z","updated":"2019-05-23T11:47:23.598Z","comments":false,"path":"categories/index.html","permalink":"https://yyyeuing.top/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-05-23T11:45:17.000Z","updated":"2019-05-23T11:46:57.851Z","comments":false,"path":"tags/index.html","permalink":"https://yyyeuing.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Attention-深度学习中的注意力机制","slug":"machine-learning","date":"2019-08-13T05:57:21.000Z","updated":"2019-08-13T14:49:26.535Z","comments":true,"path":"2019/08/13/machine-learning/","link":"","permalink":"https://yyyeuing.top/2019/08/13/machine-learning/","excerpt":"","text":"注意力模型（Attention Model）近年来被广泛的应用到NLP，图像识别，语音识别等不同程度的深度学习中。 Encoder-Decoder框架对于句子&lt;Source,Target&gt;,我们的目标是给定输入句子Source,期待通过Encoder-Decoder框架来生成目标句子Target.Source和Target可以是同一种语言，也可以是不同的语言。Source和Target分别由各自的单词序列构成。 Encoder就是对输入的句子进行编码，将输入句子通过非线性变换转换为中间语义表示C。 Decoder就是根据句子的中间语义表示C和之前已经生成的历史信息y1,y2……yi来生成i时刻要生成的单词yi;缺点：在生成目标句子的单词是，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的。这说明Source中任意单词对生成某个目标单词yi来说影响力都是相同的。相当于人眼看到画面眼中却没有焦点。举个栗子：如果将“我爱中国”这四个字翻译成英文，即“I love China”。用Encoder-Decoder框架逐步生成英文单词：“I”,“love”，“China”。在翻译“China”这个英文单词的时候，分心模型（Encoder-Decoder）里面每个中文单词对于翻译目标单词“China”贡献相同。 Attention模型Soft Attention模型针对上述栗子，如果引入Attention模型的话，在翻译“China”的时候，会体现中文单词对于翻译当前英文单词不同的影响程度，类似于给出一个概率分布值：（我：0.3）（爱：0.2）（中国：0.5）每个中文词组的概率代表了翻译当前单词“China”时，注意力分配模型分配给不同英文单词的注意力大小。Attention模型的关键在于：由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。生成目标句子单词的过程变成下面形式：那么生成目标句子某个单词，比如“China”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？？？为了方便解释，我们再次将上述的分心模型拿来，不过这次我们用RNN模型来代替它的Encoder和Decoder。那么下图👇就可以解释了Attention机制的本质思想我们将Attention机制从上面的Encoder-Decoder框架中剥离。将Source中的构成元素想象成是由一系列&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和key用来计算对应Value的权重系数。可表达为以下公式👇Attention机制的具体计算过程 阶段一：根据Query和Key计算两者的相似性或者相关性； 阶段二：对第一阶段的原始分值进行归一化处理 阶段三：根据权重系数对Value进行加权求和具体表现参考下图其中，第一阶段常见的方法有三种：求两者间的向量点积，求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值。第二阶段引入SoftMax。一方面可进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。第二阶段计算结果ai就是Value对应的权重系数，然后进行加权求和就可以得到Attention的数值。 Multi-Head AttentionGoogle提出的新概念，是Attention机制的完善。通过把Query,Key,Value通过参数矩阵映射一下，然后再做Attention，把这个过程重复做h次，最后将结果拼接起来。 Self Attention模型Attention机制发生在Target的元素Query和Source中所有元素之间。而Self Attention指的是Source内部元素之间或者Target内部元素之间发生的Attention机制，可以理解为Target=Source这种特殊情况下的注意力机制。Self Attention可以捕获同一个句子中单词之间的一些句法特征或者语义特征，会更容易捕获句子中长距离的相互依赖的特征。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://yyyeuing.top/categories/机器学习/"}],"tags":[]}]}