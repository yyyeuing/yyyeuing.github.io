{"meta":{"title":"养生少女不熬叶","subtitle":null,"description":null,"author":"养生少女不熬叶","url":"https://yyyeuing.top","root":"/yyyeuing.github.io/"},"pages":[{"title":"about","date":"2020-02-15T03:23:51.000Z","updated":"2020-02-15T03:32:16.518Z","comments":true,"path":"about/index.html","permalink":"https://yyyeuing.top/about/index.html","excerpt":"","text":""},{"title":"article","date":"2020-02-13T06:26:37.000Z","updated":"2020-02-13T06:26:37.227Z","comments":true,"path":"article/index.html","permalink":"https://yyyeuing.top/article/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-05-23T11:45:17.000Z","updated":"2019-05-23T11:46:57.851Z","comments":false,"path":"tags/index.html","permalink":"https://yyyeuing.top/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-05-23T11:45:52.000Z","updated":"2019-05-23T11:47:23.598Z","comments":false,"path":"categories/index.html","permalink":"https://yyyeuing.top/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"1-Simple Sentiment Analysis","slug":"TextClassifier","date":"2020-04-30T01:16:41.000Z","updated":"2020-05-01T14:12:43.714Z","comments":true,"path":"2020/04/30/TextClassifier/","link":"","permalink":"https://yyyeuing.top/2020/04/30/TextClassifier/","excerpt":"电影评论情感分类最简版","text":"电影评论情感分类最简版 Simple Sentiment Analysis数据处理： 下载数据集，切分成训练集，验证集，测试集。 建立词汇表。由于词汇过多会导致训练时间过长，需要进行简化，可以通过选取最常出现的n个词汇，或者舍弃出现次数少于m次的词汇。每次输入是一批句子，句子太短的用pad进行填充，其次用unk来填充被舍弃的词汇。 创建迭代器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchfrom torchtext import dataimport spacy#随机种子繁衍SEED = 1234torch.manual_seed(SEED)torch.backends.cudnn.deterministic = TrueTEXT = data.Field(tokenize=&apos;spacy&apos;,tokenizer_language=&apos;en_core_web_sm&apos;)LABEL = data.LabelField(dtype=torch.float)#下载数据集切分成训练集和测试集from torchtext import datasetstrain_data,test_data = datasets.IMDB.splits(TEXT,LABEL)print(f&apos;Number of training examples: &#123;len(train_data)&#125;&apos;)print(f&apos;Number of testing examples: &#123;len(test_data)&#125;&apos;)#print(vars(train_data.examples[0]))#将训练集切分为训练集和验证集import randomtrain_data,valid_data = train_data.split(random_state = random.seed(SEED))print(f&apos;Number of training examples: &#123;len(train_data)&#125;&apos;)print(f&apos;Number of validation examples:&#123;len(valid_data)&#125;&apos;)print(f&apos;Number of testing examples: &#123;len(test_data)&#125;&apos;)#建立词汇表MAX_VOCAB_SIZE = 25000TEXT.build_vocab(train_data,max_size = MAX_VOCAB_SIZE)LABEL.build_vocab(train_data)#输出“Unique tokens in TEXT vocabulary: 25002”是由于添加了&lt;unk&gt;和&lt;pad&gt;#&lt;unk&gt;是补充丢弃的词汇#&lt;pad&gt;是由于每次是输入一批句子，句子的长度要求一样，短句子会被填充print(f&apos;Unique tokens in TEXT vocabulary: &#123;len(TEXT.vocab)&#125;&apos;)print(f&apos;Unique tokens in TEXT vocabulary: &#123;len(LABEL.vocab)&#125;&apos;)print(f&apos;The most common words in the vocabulary and their frequencies:&#123;TEXT.vocab.freqs.most_common(20)&#125;&apos;)print(TEXT.vocab.itos[:10])# stoi (string to int) , itos (int to string)print(LABEL.vocab.stoi) #0:negative,1:positive###迭代BATCH_SIZE = 64device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)train_iterator,valid_iterator,test_iterator = data.BucketIterator.splits( (train_data,valid_data,test_data), batch_size=BATCH_SIZE, device = device)#返回一批示例，其中每个示例的长度都相似，从而最小化每个示例的填充量。 定义并训练模型Three layers:embeddiong layer,RNN,linear layers Embedding layer:transform our sparse one-hot vector into a dense embedding vector.(a single fully connected layer) RNN:take in our dense vector and the previous hidden state ht-1(to calculate the next hidden state ht)RNN(循环神经网络)：一个序列当前的输出与前面的输出有关 Linear layer:take the final hidden state and feed it through a fully connected layer,transforming it to the correct output dimension. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#搭建模型import torch.nn as nnclass RNN(nn.Module): def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim): super().__init__() self.embedding = nn.Embedding(input_dim,embedding_dim) self.rnn = nn.RNN(embedding_dim,hidden_dim) self.fc = nn.Linear(hidden_dim,output_dim) def forward(self,text): #text = [sent len,batch size] #embedded = [sent len,batch size,emb dim] #output = [sent len, batch size,hid dim] #hidden - [1,batch size,hid dim] embedded = self.embedding(text) output,hidden = self.rnn(embedded) assert torch.equal(output[-1,:,:],hidden.squeeze(0)) return self.fc(hidden.squeeze(0))INPUT_DIM = len(TEXT.vocab)EMBEDDING_DIM = 100HIDDEN_DIM = 256OUTPUT_DIM = 1model = RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM).to(device)#定义函数求可训练的参数数量def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f&apos;The model has &#123;count_parameters(model):,&#125; trainable paremeters&apos;)#训练模型import torch.optim as optimoptimizer = optim.SGD(model.parameters(),lr=1e-3)#优化器，更新模块参数；随机梯度下降法，SGDcriterion = nn.BCEWithLogitsLoss().to(device)#损失函数，BCEWithLogitsLoss同时执行sigmoid和二元交叉熵步骤。def binary_accuracy(preds,y): #将预测结果四舍五入到最近结果 rounded_preds = torch.round(torch.sigmoid(preds)) correct = (rounded_preds == y).float()#转换为浮点数除法 acc = correct.sum()/len(correct) return accdef train(model,iterator,optimizer,criterion): epoch_loss = 0 epoch_acc = 0 model.train()#让dropout和BN生效，此处没用到 for batch in iterator: optimizer.zero_grad() predictions = model(batch.text).squeeze(1) loss = criterion(predictions,batch.label) acc = binary_accuracy(predictions,batch.label) loss.backward() #计算每个参数的梯度 optimizer.step() epoch_loss += loss.item() #.item()从一个只包含一个值的张量中提取标量。 epoch_acc += acc.item() return epoch_loss/len(iterator),epoch_acc/len(iterator)def evaluate(model,iterator,criterion): epoch_loss = 0 epoch_ass = 0 model.eval() with torch.no_grad(): #不计算梯度 for batch in iterator: predictions = model(batch.text).squeeze(1) loss = criterion(predictions,batch.label) acc = criterion(predictions,batch.label) epoch_loss += loss.item() epoch_ass += acc.item() return epoch_loss/len(iterator),epoch_ass/len(iterator)import timedef epoch_time(start_time,end_time): elapsed_time = end_time-start_time elapsed_mins = int(elapsed_time/60) elapsed_secs = int(elapsed_time-(elapsed_mins*60)) return elapsed_mins,elapsed_secsN_EPOCHS = 5best_valid_loss = float(&apos;inf&apos;)for epoch in range(N_EPOCHS): start_time = time.time() train_loss,train_acc = train(model,train_iterator,optimizer,criterion) valid_loss,valid_acc = evaluate(model,valid_iterator,criterion) end_time = time.time() epoch_mins,epoch_secs = epoch_time(start_time,end_time) if valid_loss &lt; best_valid_loss: best_valid_loss = valid_loss torch.save(model.state_dict(),&apos;tut1-model.pt&apos;) print(f&apos;Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;&apos;) print(f&apos;\\tTrain Loss: &#123;train_loss:.3f&#125; | Train ACC: &#123;train_acc * 100 :.2f&#125;%&apos;) print(f&apos;\\tVal Loss: &#123;valid_loss:.3f&#125; | Val ACC: &#123;valid_acc * 100 :.2f&#125;%&apos;)model.load_state_dict(torch.load(&apos;tut1-model.pt&apos;))test_loss,test_acc = evaluate(model,test_iterator,criterion)print(f&apos;Test Loss: &#123;test_loss:.3f&#125; | Test Acc: &#123;test_acc *100 :.2f&#125;%&apos;) 结果123456789101112131415161718192021222324252627282930313233Number of training examples: 25000Number of testing examples: 25000Number of training examples: 17500Number of validation examples:7500Number of testing examples: 25000Unique tokens in TEXT vocabulary: 25002Unique tokens in TEXT vocabulary: 2The most common words in the vocabulary and their frequencies:[(&apos;the&apos;, 203566), (&apos;,&apos;, 192495), (&apos;.&apos;, 165618), (&apos;and&apos;, 109442), (&apos;a&apos;, 109116), (&apos;of&apos;, 100702), (&apos;to&apos;, 93766), (&apos;is&apos;, 76328), (&apos;in&apos;, 61255), (&apos;I&apos;, 54004), (&apos;it&apos;, 53508), (&apos;that&apos;, 49187), (&apos;&quot;&apos;, 44282), (&quot;&apos;s&quot;, 43329), (&apos;this&apos;, 42445), (&apos;-&apos;, 36692), (&apos;/&gt;&lt;br&apos;, 35752), (&apos;was&apos;, 35034), (&apos;as&apos;, 30384), (&apos;with&apos;, 29774)][&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;and&apos;, &apos;a&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]defaultdict(None, &#123;&apos;neg&apos;: 0, &apos;pos&apos;: 1&#125;)The model has 2,592,105 trainable paremetersEpoch: 01 | Epoch Time: 0m 52 Train Loss: 0.694 | Train ACC: 50.28% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 02 | Epoch Time: 0m 54 Train Loss: 0.693 | Train ACC: 49.87% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 03 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.18% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 04 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 49.77% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 05 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.11% Val Loss: 0.697 | Val ACC: 69.71%Test Loss: 0.709 | Test Acc: 70.92% 附录遇到的一些坑：anaconda中安装spacy后，出现Warning: no model found for ‘en’网上有很多教程可以解决，通常可以用以下代码解决1python -m spacy download en_core_web_sm 但是也会存在一直下载不下来的情况，这时可以自己从官网上下载。最重要的一点就是下载时要看好版本，网上有很多教程推荐的版本已经很老的，如果下载的还是那个“老版本”，可能和我一样会出现以下报错re.error: bad escape \\p at position 257所以下载时一定要看好自己spacy的版本，然后再去下载对应的en_core_web_sm版本。最后输入1pip install en_core_web_sm-2.2.5.tar.gz 文章参考 1 - Simple Sentiment Analysis.ipynb","categories":[],"tags":[{"name":"Sentiment Analysis","slug":"Sentiment-Analysis","permalink":"https://yyyeuing.top/tags/Sentiment-Analysis/"},{"name":"nlp","slug":"nlp","permalink":"https://yyyeuing.top/tags/nlp/"}]},{"title":"那些年一起看过的动漫","slug":"cartoon","date":"2020-02-14T12:31:51.000Z","updated":"2020-05-01T14:38:53.927Z","comments":true,"path":"2020/02/14/cartoon/","link":"","permalink":"https://yyyeuing.top/2020/02/14/cartoon/","excerpt":"小铭同学是一个资深的动漫迷，为避免和熬夜少女吃饭的时候玩手机，小铭同学开始推荐动漫。此文用来记录推荐那些年小铭同学和熬夜少女看过的动漫✌","text":"小铭同学是一个资深的动漫迷，为避免和熬夜少女吃饭的时候玩手机，小铭同学开始推荐动漫。此文用来记录推荐那些年小铭同学和熬夜少女看过的动漫✌ 狐妖小红娘心爱指数：⭐⭐⭐⭐⭐ 这是我看的第一部动漫，刚开始看觉得有点奇怪？感觉有点沙雕，不是特别喜欢（之前一直没看过动漫，说不清的感觉）但看到后面越看越上头，真香现场哈哈哈。狐妖分为很多篇章。每个章节相互独立又互相关联，现在已经看完的有下沙、尾声、王权、月红、北山、千颜、南国、竹业。除了感人的情情爱爱，动漫里构造的世界也让人很着迷。狐妖尾声篇也来了！呜呜呜，疫情待在家不能和小铭同学一起看了(づ╥﹏╥)づ 王权：万剑穿心终不悔，相视一笑轻王权。 月红：前世你唤我一声妖仙姐姐，今生我叫你一声道士哥哥。 竹业：七月初七，淮水竹亭，鞘笛相依，无怨无悔。霸业和淮竹是我的最爱😍 工作细胞心爱指数：⭐⭐⭐⭐ 看完后经常就是“塔塔那酷~”，血小板真的好可爱，红细胞很蠢萌，白细胞很厉害但又有点小闷骚的可爱。里面的每个细胞不眠不休各司其职。通过感冒，过敏，中暑等等症状，将细胞真人化，展示出人体面对疾病时得样貌。（这段怎么写的这么官方哈哈哈）看完感觉自己是真的好腻害o(￣▽￣)ブ，啊不，是我身体里的细胞好厉害！但是每一集动漫的主人公都会生一次病，真是让人感慨这个小人儿有点惨惨呐，生病次数有点多呐，而且据说这个人儿是个小孩 罗小黑战记心爱指数：⭐⭐⭐（剧很好！就是更新太慢了！！！） 猫妖小黑盗取天明珠被谛听发现，被打回原形重伤而逃，在流落街头之时被罗小白带回家，起名罗小黑。它极通人性，会蹲马桶，不吃猫粮，长长的尾巴可以分裂成多个“黑咻”。。。（源自百度百科O(∩_∩)O）画风很萌萌哒，温馨又有趣。电影和动漫都看完了，超爱的一部，等不到更新真的让人抓狂。 辉夜大小姐想让我告白心爱指数：⭐⭐⭐⭐ 这部番刚刚开始看，真的有点看不下去，感觉有点无聊。但是！看到后面简直就是欲罢不能呀，啊啦啊啦~豪卡哇伊阔多~苍白的剧情介绍：两个傲娇学霸爱恋对方，为了面子各种算计对方对自己进行告白的故事。有的时候看的我真的很着急，想要手动按头╮(╯▽╰)╭（里面的藤原书记绝对是一大亮点，超级可爱，但也有点小腹黑，我很爱呐）辉夜大小姐第二季也来了！还是疫情，我还是不能和小铭同学一起看(┬＿┬)，希望疫情快快结束，大家都要健健康康的，等到胜利的时候，大家伙儿又可以一起开心的聚在一起啦！辉夜我一定要留着和小铭同学一起看！ 紫罗兰永恒花园心爱指数：⭐⭐⭐⭐⭐ 薇尔莉特·伊芙加登真的好美，人美，性格美。幼时被基尔伯特少佐捡回家，作为一个军人她有非凡的战斗力，却不谙人事。不知爱为何物，她只想做一个优秀的军人，做少佐得力的左膀右臂。为了明白少佐口中的“爱”，她努力的想成为一名出色的“自动手记人偶”，在一次次的代写书信的过程中，从没有心到拥有一颗柔软的心。里面有很多感人的故事，非常适合一个人在安静的环境下看，很治愈。最近看B站，有弹幕说少佐没有死！结局是好的！我太开心了！（虽然不知道是不是真的，就算是假的，我也要脑补是真的。） 鬼灭之刃心爱指数：⭐⭐⭐⭐⭐ 一部热血动漫，画风很喜欢，题材也很喜欢。炭治郎的家人被恶鬼吃了，母亲和四个弟弟妹妹都被杀死，唯一生还的妹妹祢豆子也变成了鬼。为了让妹妹祢豆子变回人类，为了讨伐杀害家人的恶鬼，炭治郎成为了“杀鬼队”的一员。总的来说就是热血少年为救妹踏上灭鬼之路的故事。超级超级喜欢！！！最近几期的动漫更新好像坐过山车，炭治郎变鬼了？炭治郎又不是鬼了？靠着微博热搜看完了大结局，意料之外情理之中罢了。期待剧版。 刺客五六七心爱指数：⭐⭐⭐⭐⭐ 阿珍爱上了阿强。 今天我就要带她走，我看谁敢拦我！ 你不说出来 对方就永远不知道你的心意 每一个引得观众发笑的人物，其实都有一个悲情的内核。 不拼尽全力去试一下，又怎么会知道啊 我是刺客五六七，刺客排行榜一万七千三百六十九位，最擅长剪空气刘海，一直以优质的服务和亲民的价格深受村民的喜爱，目前的情感状况是单身，不如一起去喝杯东西交流..明明是一部搞笑的番，却又有很多让人感动的地方。","categories":[],"tags":[{"name":"动漫","slug":"动漫","permalink":"https://yyyeuing.top/tags/动漫/"},{"name":"休闲","slug":"休闲","permalink":"https://yyyeuing.top/tags/休闲/"}]}]}