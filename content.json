{"meta":{"title":"养生少女不熬叶","subtitle":null,"description":null,"author":"养生少女不熬叶","url":"https://yyyeuing.top","root":"/yyyeuing.github.io/"},"pages":[{"title":"categories","date":"2019-05-23T11:45:52.000Z","updated":"2019-05-23T11:47:23.598Z","comments":false,"path":"categories/index.html","permalink":"https://yyyeuing.top/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2020-02-15T03:23:51.000Z","updated":"2020-02-15T03:32:16.518Z","comments":true,"path":"about/index.html","permalink":"https://yyyeuing.top/about/index.html","excerpt":"","text":""},{"title":"article","date":"2020-02-13T06:26:37.000Z","updated":"2020-02-13T06:26:37.227Z","comments":true,"path":"article/index.html","permalink":"https://yyyeuing.top/article/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-05-23T11:45:17.000Z","updated":"2019-05-23T11:46:57.851Z","comments":false,"path":"tags/index.html","permalink":"https://yyyeuing.top/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2.Transformers for text classification","slug":"bert-clue","date":"2020-06-27T13:44:28.000Z","updated":"2020-09-10T01:43:40.353Z","comments":true,"path":"2020/06/27/bert-clue/","link":"","permalink":"https://yyyeuing.top/2020/06/27/bert-clue/","excerpt":"基于bert实现的文本分类","text":"基于bert实现的文本分类 数据处理 数据集使用TNEWS下载地址 数据集包括：训练集(53,360)，验证集(10,000)，测试集(10,000) 每条数据包括三个属性：分类的ID，分类的名称，新闻字符串。其中共包含15个标签。 eg: {“label”: “108”, “label_desc”: “news_edu”, “sentence”: “上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？”, “keywords”: “”} 定义InputExample类，其中包括：guid，text_a,text_b,label着四个属性。由于是单行文本分类的问题，text_b用不到，定义为None。 1234567# 读取json文件class InputExample(object): def __init__(self,guid,text_a,text_b&#x3D;None,lable &#x3D; None): self.guid &#x3D; guid self.text_a &#x3D; text_a self.text_b &#x3D; text_b self.label &#x3D; lable 按行读取数据，每行读取到的[sentence]部分即为分类的语句，将其转换成一个一个的token后，再将token转换成对应的id。最后返回的是一个列表examples，里面包含每条处理后的数据。 1234567891011121314151617181920212223# 读取json文件def read_json(input_file): with open(input_file, &#39;r&#39;, encoding&#x3D;&#39;UTF-8&#39;) as f: reader &#x3D; f.readlines() lines &#x3D; [] for line in reader: lines.append(json.loads(line.strip())) # print(lines) return linesdef get_examples(lines,set_type): examples &#x3D; [] for (i, line) in enumerate (lines): guid &#x3D; &quot;%s-%s&quot; % (set_type, i) text_a &#x3D; tokenizer.convert_tokens_to_ids(tokenizer.tokenize(lines[i][&#39;sentence&#39;])) text_b &#x3D; None label &#x3D; str(line[&#39;label&#39;]) if set_type !&#x3D; &#39;test&#39; else &quot;100&quot; examples.append(InputExample(guid &#x3D; guid,text_a&#x3D;text_a,text_b&#x3D;text_b, lable&#x3D; label)) # print(len(examples)) return examplesdef get_any_examples(data_dir): return get_examples(read_json(data_dir), os.path.splitext(data_dir)[0]) 网络模型使用bert预训练模型，具体可参考huggingface官方文档 . bert模型类的输入主要分以下四个部分： input_ids:文本对应的id序列,shape=[batch_size, sequence_length] attention_mask:文本对应的pad标记序列,1表示该位置未被填充，0表示被填充。shape=[batch_size, sequence_length], token_type_ids:用于区分文本中两个句子的标记序列,0表示相应token属于第一个句子，1表示相应token属于第二个句子。shape=[batch_size, sequence_length] 代码实现：在bert中，限定输入的长度不能超过512，故定义max_length=512。不及512的地方补0，超过的进行裁剪。定义四个列表：id_list、attention_masks_list、token_type_ids_list、labels_list，将每条数据转换得来的输入存入到相应的列表，最后将列表存入data字典中。 12345678910111213141516171819202122232425262728293031323334353637383940414243def preprocess(data_dir, max_length &#x3D; 512): &quot;&quot;&quot; 处理数据 :param raw_data_fn: 原始数据文件名 :return: &quot;&quot;&quot; examples &#x3D; get_any_examples(data_dir) id_list &#x3D; [] attention_masks_list &#x3D; [] token_type_ids_list &#x3D; [] labels_list &#x3D; [] for example in examples: input_ids &#x3D; example.text_a label_list &#x3D; get_lables() label_map &#x3D; &#123;label: i for i ,label in enumerate(label_list)&#125; label &#x3D; label_map[example.label] if len(input_ids) &gt; max_length: token &#x3D; tokenizer.tokenize(input_ids) input_ids &#x3D; token[:max_length-2] input_len &#x3D; len(input_ids) padding_length &#x3D; max_length-input_len attention_masks &#x3D; [1] * input_len + [0] * padding_length input_ids &#x3D; input_ids + ([0] * padding_length) token_type_ids &#x3D;[0] * max_length id_list.append(input_ids) attention_masks_list.append(attention_masks) token_type_ids_list.append(token_type_ids) labels_list.append(label) data &#x3D; &#123;&#39;input_ids&#39;:id_list, &#39;attention_masks&#39;:attention_masks_list, &#39;token_type_ids&#39;:token_type_ids_list, &#39;input_labels&#39;:labels_list&#125; return data bert模型调用该任务用到的是transformers提供的BertForSequenceClassification1234# 加载模型及配置方法 bert_config &#x3D; BertConfig.from_pretrained(model_name, num_labels&#x3D;15) # 头条文本分类数据集为15类 model &#x3D; BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path&#x3D;model_name, config&#x3D;bert_config, cache_dir&#x3D;cache_dir) optimizer &#x3D; torch.optim.Adam(model.parameters(), lr&#x3D;lr) 模型训练该任务是单行文本问题，只需要输入input_ids和labels即可。output[0]为loss。(查看BertForSequenceClassification类函数可更清楚的了解模型的输入和输出) 123456789101112131415161718192021222324252627282930313233def train(model, optimizer, data_loader): min_loss &#x3D; float(&#39;inf&#39;) for e in range(epochs): for step,batch in enumerate(data_loader): model.zero_grad() model.train() inputs &#x3D; &#123;&#39;input_ids&#39;:batch[0], &#39;attention_mask&#39;:batch[1], &#39;token_tpye_ids&#39;:batch[2], &#39;labels&#39;:batch[3] &#125; output &#x3D; model(input_ids &#x3D; inputs[&#39;input_ids&#39;], labels &#x3D; inputs[&#39;labels&#39;]) # print(output[0]) train_loss &#x3D; output[0] train_loss &#x3D; train_loss &#x2F; accumulation_steps train_loss.backward(retain_graph&#x3D;True) # 每八次更新一下网络中的参数 if (step+1) % accumulation_steps &#x3D;&#x3D; 0: optimizer.step() optimizer.zero_grad() if (step+1) % accumulation_steps &#x3D;&#x3D; 1: print(&#39;Train Epoch: &#123;&#125; [&#123;&#125;&#x2F;&#123;&#125;] || train_loss: &#123;:.6f&#125;&#39;.format( e+1, step * batch_size, len(data_loader.dataset), train_loss.item() )) print(&#39;Train Epoch: &#123;&#125; || train_loss:&#123;:.6f&#125;.&#39;.format(e+1,train_loss.item())) if train_loss &lt; min_loss: min_loss &#x3D; train_loss torch.save(model.state_dict(), path) 附： bert超详细讲解播客(๑•̀ㅂ•́)و✧CLUE benchmark代码","categories":[],"tags":[{"name":"文本分类","slug":"文本分类","permalink":"https://yyyeuing.top/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"nlp","slug":"nlp","permalink":"https://yyyeuing.top/tags/nlp/"}]},{"title":"1-Simple Sentiment Analysis","slug":"TextClassifier","date":"2020-04-30T01:16:41.000Z","updated":"2020-06-27T14:11:13.447Z","comments":true,"path":"2020/04/30/TextClassifier/","link":"","permalink":"https://yyyeuing.top/2020/04/30/TextClassifier/","excerpt":"电影评论情感分类最简版","text":"电影评论情感分类最简版 Simple Sentiment Analysis数据处理 下载数据集，切分成训练集，验证集，测试集。 建立词汇表。由于词汇过多会导致训练时间过长，需要进行简化，可以通过选取最常出现的n个词汇，或者舍弃出现次数少于m次的词汇。每次输入是一批句子，句子太短的用pad进行填充，其次用unk来填充被舍弃的词汇。 创建迭代器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchfrom torchtext import dataimport spacy#随机种子繁衍SEED &#x3D; 1234torch.manual_seed(SEED)torch.backends.cudnn.deterministic &#x3D; TrueTEXT &#x3D; data.Field(tokenize&#x3D;&#39;spacy&#39;,tokenizer_language&#x3D;&#39;en_core_web_sm&#39;)LABEL &#x3D; data.LabelField(dtype&#x3D;torch.float)#下载数据集切分成训练集和测试集from torchtext import datasetstrain_data,test_data &#x3D; datasets.IMDB.splits(TEXT,LABEL)print(f&#39;Number of training examples: &#123;len(train_data)&#125;&#39;)print(f&#39;Number of testing examples: &#123;len(test_data)&#125;&#39;)#print(vars(train_data.examples[0]))#将训练集切分为训练集和验证集import randomtrain_data,valid_data &#x3D; train_data.split(random_state &#x3D; random.seed(SEED))print(f&#39;Number of training examples: &#123;len(train_data)&#125;&#39;)print(f&#39;Number of validation examples:&#123;len(valid_data)&#125;&#39;)print(f&#39;Number of testing examples: &#123;len(test_data)&#125;&#39;)#建立词汇表MAX_VOCAB_SIZE &#x3D; 25000TEXT.build_vocab(train_data,max_size &#x3D; MAX_VOCAB_SIZE)LABEL.build_vocab(train_data)#输出“Unique tokens in TEXT vocabulary: 25002”是由于添加了&lt;unk&gt;和&lt;pad&gt;#&lt;unk&gt;是补充丢弃的词汇#&lt;pad&gt;是由于每次是输入一批句子，句子的长度要求一样，短句子会被填充print(f&#39;Unique tokens in TEXT vocabulary: &#123;len(TEXT.vocab)&#125;&#39;)print(f&#39;Unique tokens in TEXT vocabulary: &#123;len(LABEL.vocab)&#125;&#39;)print(f&#39;The most common words in the vocabulary and their frequencies:&#123;TEXT.vocab.freqs.most_common(20)&#125;&#39;)print(TEXT.vocab.itos[:10])# stoi (string to int) , itos (int to string)print(LABEL.vocab.stoi) #0:negative,1:positive###迭代BATCH_SIZE &#x3D; 64device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)train_iterator,valid_iterator,test_iterator &#x3D; data.BucketIterator.splits( (train_data,valid_data,test_data), batch_size&#x3D;BATCH_SIZE, device &#x3D; device)#返回一批示例，其中每个示例的长度都相似，从而最小化每个示例的填充量。 定义并训练模型Three layers:embeddiong layer,RNN,linear layers Embedding layer:transform our sparse one-hot vector into a dense embedding vector.(a single fully connected layer) RNN:take in our dense vector and the previous hidden state ht-1(to calculate the next hidden state ht)RNN(循环神经网络)：一个序列当前的输出与前面的输出有关 Linear layer:take the final hidden state and feed it through a fully connected layer,transforming it to the correct output dimension. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#搭建模型import torch.nn as nnclass RNN(nn.Module): def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim): super().__init__() self.embedding &#x3D; nn.Embedding(input_dim,embedding_dim) self.rnn &#x3D; nn.RNN(embedding_dim,hidden_dim) self.fc &#x3D; nn.Linear(hidden_dim,output_dim) def forward(self,text): #text &#x3D; [sent len,batch size] #embedded &#x3D; [sent len,batch size,emb dim] #output &#x3D; [sent len, batch size,hid dim] #hidden - [1,batch size,hid dim] embedded &#x3D; self.embedding(text) output,hidden &#x3D; self.rnn(embedded) assert torch.equal(output[-1,:,:],hidden.squeeze(0)) return self.fc(hidden.squeeze(0))INPUT_DIM &#x3D; len(TEXT.vocab)EMBEDDING_DIM &#x3D; 100HIDDEN_DIM &#x3D; 256OUTPUT_DIM &#x3D; 1model &#x3D; RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM).to(device)#定义函数求可训练的参数数量def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f&#39;The model has &#123;count_parameters(model):,&#125; trainable paremeters&#39;)#训练模型import torch.optim as optimoptimizer &#x3D; optim.SGD(model.parameters(),lr&#x3D;1e-3)#优化器，更新模块参数；随机梯度下降法，SGDcriterion &#x3D; nn.BCEWithLogitsLoss().to(device)#损失函数，BCEWithLogitsLoss同时执行sigmoid和二元交叉熵步骤。def binary_accuracy(preds,y): #将预测结果四舍五入到最近结果 rounded_preds &#x3D; torch.round(torch.sigmoid(preds)) correct &#x3D; (rounded_preds &#x3D;&#x3D; y).float()#转换为浮点数除法 acc &#x3D; correct.sum()&#x2F;len(correct) return accdef train(model,iterator,optimizer,criterion): epoch_loss &#x3D; 0 epoch_acc &#x3D; 0 model.train()#让dropout和BN生效，此处没用到 for batch in iterator: optimizer.zero_grad() predictions &#x3D; model(batch.text).squeeze(1) loss &#x3D; criterion(predictions,batch.label) acc &#x3D; binary_accuracy(predictions,batch.label) loss.backward() #计算每个参数的梯度 optimizer.step() epoch_loss +&#x3D; loss.item() #.item()从一个只包含一个值的张量中提取标量。 epoch_acc +&#x3D; acc.item() return epoch_loss&#x2F;len(iterator),epoch_acc&#x2F;len(iterator)def evaluate(model,iterator,criterion): epoch_loss &#x3D; 0 epoch_ass &#x3D; 0 model.eval() with torch.no_grad(): #不计算梯度 for batch in iterator: predictions &#x3D; model(batch.text).squeeze(1) loss &#x3D; criterion(predictions,batch.label) acc &#x3D; criterion(predictions,batch.label) epoch_loss +&#x3D; loss.item() epoch_ass +&#x3D; acc.item() return epoch_loss&#x2F;len(iterator),epoch_ass&#x2F;len(iterator)import timedef epoch_time(start_time,end_time): elapsed_time &#x3D; end_time-start_time elapsed_mins &#x3D; int(elapsed_time&#x2F;60) elapsed_secs &#x3D; int(elapsed_time-(elapsed_mins*60)) return elapsed_mins,elapsed_secsN_EPOCHS &#x3D; 5best_valid_loss &#x3D; float(&#39;inf&#39;)for epoch in range(N_EPOCHS): start_time &#x3D; time.time() train_loss,train_acc &#x3D; train(model,train_iterator,optimizer,criterion) valid_loss,valid_acc &#x3D; evaluate(model,valid_iterator,criterion) end_time &#x3D; time.time() epoch_mins,epoch_secs &#x3D; epoch_time(start_time,end_time) if valid_loss &lt; best_valid_loss: best_valid_loss &#x3D; valid_loss torch.save(model.state_dict(),&#39;tut1-model.pt&#39;) print(f&#39;Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;&#39;) print(f&#39;\\tTrain Loss: &#123;train_loss:.3f&#125; | Train ACC: &#123;train_acc * 100 :.2f&#125;%&#39;) print(f&#39;\\tVal Loss: &#123;valid_loss:.3f&#125; | Val ACC: &#123;valid_acc * 100 :.2f&#125;%&#39;)model.load_state_dict(torch.load(&#39;tut1-model.pt&#39;))test_loss,test_acc &#x3D; evaluate(model,test_iterator,criterion)print(f&#39;Test Loss: &#123;test_loss:.3f&#125; | Test Acc: &#123;test_acc *100 :.2f&#125;%&#39;) 结果 123456789101112131415161718192021222324252627282930313233Number of training examples: 25000Number of testing examples: 25000Number of training examples: 17500Number of validation examples:7500Number of testing examples: 25000Unique tokens in TEXT vocabulary: 25002Unique tokens in TEXT vocabulary: 2The most common words in the vocabulary and their frequencies:[(&#39;the&#39;, 203566), (&#39;,&#39;, 192495), (&#39;.&#39;, 165618), (&#39;and&#39;, 109442), (&#39;a&#39;, 109116), (&#39;of&#39;, 100702), (&#39;to&#39;, 93766), (&#39;is&#39;, 76328), (&#39;in&#39;, 61255), (&#39;I&#39;, 54004), (&#39;it&#39;, 53508), (&#39;that&#39;, 49187), (&#39;&quot;&#39;, 44282), (&quot;&#39;s&quot;, 43329), (&#39;this&#39;, 42445), (&#39;-&#39;, 36692), (&#39;&#x2F;&gt;&lt;br&#39;, 35752), (&#39;was&#39;, 35034), (&#39;as&#39;, 30384), (&#39;with&#39;, 29774)][&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;the&#39;, &#39;,&#39;, &#39;.&#39;, &#39;and&#39;, &#39;a&#39;, &#39;of&#39;, &#39;to&#39;, &#39;is&#39;]defaultdict(None, &#123;&#39;neg&#39;: 0, &#39;pos&#39;: 1&#125;)The model has 2,592,105 trainable paremetersEpoch: 01 | Epoch Time: 0m 52 Train Loss: 0.694 | Train ACC: 50.28% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 02 | Epoch Time: 0m 54 Train Loss: 0.693 | Train ACC: 49.87% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 03 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.18% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 04 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 49.77% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 05 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.11% Val Loss: 0.697 | Val ACC: 69.71%Test Loss: 0.709 | Test Acc: 70.92% 附录遇到的一些坑：anaconda中安装spacy后，出现Warning: no model found for ‘en’网上有很多教程可以解决，通常可以用以下代码解决 1python -m spacy download en_core_web_sm 但是也会存在一直下载不下来的情况，这时可以自己从官网上下载。最重要的一点就是下载时要看好版本，网上有很多教程推荐的版本已经很老的，如果下载的还是那个“老版本”，可能和我一样会出现以下报错re.error: bad escape \\p at position 257所以下载时一定要看好自己spacy的版本，然后再去下载对应的en_core_web_sm版本。最后输入 1pip install en_core_web_sm-2.2.5.tar.gz 文章参考 1 - Simple Sentiment Analysis.ipynb","categories":[],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://yyyeuing.top/tags/nlp/"},{"name":"Sentiment Analysis","slug":"Sentiment-Analysis","permalink":"https://yyyeuing.top/tags/Sentiment-Analysis/"}]}]}