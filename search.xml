<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2Fyyyeuing.github.io%2F2020%2F05%2F07%2Ftest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[1-Simple Sentiment Analysis]]></title>
    <url>%2Fyyyeuing.github.io%2F2020%2F04%2F30%2FTextClassifier%2F</url>
    <content type="text"><![CDATA[ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»æœ€ç®€ç‰ˆ Simple Sentiment Analysisæ•°æ®å¤„ç†ï¼š ä¸‹è½½æ•°æ®é›†ï¼Œåˆ‡åˆ†æˆè®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ã€‚ å»ºç«‹è¯æ±‡è¡¨ã€‚ç”±äºŽè¯æ±‡è¿‡å¤šä¼šå¯¼è‡´è®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œéœ€è¦è¿›è¡Œç®€åŒ–ï¼Œå¯ä»¥é€šè¿‡é€‰å–æœ€å¸¸å‡ºçŽ°çš„nä¸ªè¯æ±‡ï¼Œæˆ–è€…èˆå¼ƒå‡ºçŽ°æ¬¡æ•°å°‘äºŽmæ¬¡çš„è¯æ±‡ã€‚æ¯æ¬¡è¾“å…¥æ˜¯ä¸€æ‰¹å¥å­ï¼Œå¥å­å¤ªçŸ­çš„ç”¨padè¿›è¡Œå¡«å……ï¼Œå…¶æ¬¡ç”¨unkæ¥å¡«å……è¢«èˆå¼ƒçš„è¯æ±‡ã€‚ åˆ›å»ºè¿­ä»£å™¨ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchfrom torchtext import dataimport spacy#éšæœºç§å­ç¹è¡SEED &#x3D; 1234torch.manual_seed(SEED)torch.backends.cudnn.deterministic &#x3D; TrueTEXT &#x3D; data.Field(tokenize&#x3D;&#39;spacy&#39;,tokenizer_language&#x3D;&#39;en_core_web_sm&#39;)LABEL &#x3D; data.LabelField(dtype&#x3D;torch.float)#ä¸‹è½½æ•°æ®é›†åˆ‡åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†from torchtext import datasetstrain_data,test_data &#x3D; datasets.IMDB.splits(TEXT,LABEL)print(f&#39;Number of training examples: &#123;len(train_data)&#125;&#39;)print(f&#39;Number of testing examples: &#123;len(test_data)&#125;&#39;)#print(vars(train_data.examples[0]))#å°†è®­ç»ƒé›†åˆ‡åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†import randomtrain_data,valid_data &#x3D; train_data.split(random_state &#x3D; random.seed(SEED))print(f&#39;Number of training examples: &#123;len(train_data)&#125;&#39;)print(f&#39;Number of validation examples:&#123;len(valid_data)&#125;&#39;)print(f&#39;Number of testing examples: &#123;len(test_data)&#125;&#39;)#å»ºç«‹è¯æ±‡è¡¨MAX_VOCAB_SIZE &#x3D; 25000TEXT.build_vocab(train_data,max_size &#x3D; MAX_VOCAB_SIZE)LABEL.build_vocab(train_data)#è¾“å‡ºâ€œUnique tokens in TEXT vocabulary: 25002â€æ˜¯ç”±äºŽæ·»åŠ äº†&lt;unk&gt;å’Œ&lt;pad&gt;#&lt;unk&gt;æ˜¯è¡¥å……ä¸¢å¼ƒçš„è¯æ±‡#&lt;pad&gt;æ˜¯ç”±äºŽæ¯æ¬¡æ˜¯è¾“å…¥ä¸€æ‰¹å¥å­ï¼Œå¥å­çš„é•¿åº¦è¦æ±‚ä¸€æ ·ï¼ŒçŸ­å¥å­ä¼šè¢«å¡«å……print(f&#39;Unique tokens in TEXT vocabulary: &#123;len(TEXT.vocab)&#125;&#39;)print(f&#39;Unique tokens in TEXT vocabulary: &#123;len(LABEL.vocab)&#125;&#39;)print(f&#39;The most common words in the vocabulary and their frequencies:&#123;TEXT.vocab.freqs.most_common(20)&#125;&#39;)print(TEXT.vocab.itos[:10])# stoi (string to int) , itos (int to string)print(LABEL.vocab.stoi) #0:negative,1:positive###è¿­ä»£BATCH_SIZE &#x3D; 64device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)train_iterator,valid_iterator,test_iterator &#x3D; data.BucketIterator.splits( (train_data,valid_data,test_data), batch_size&#x3D;BATCH_SIZE, device &#x3D; device)#è¿”å›žä¸€æ‰¹ç¤ºä¾‹ï¼Œå…¶ä¸­æ¯ä¸ªç¤ºä¾‹çš„é•¿åº¦éƒ½ç›¸ä¼¼ï¼Œä»Žè€Œæœ€å°åŒ–æ¯ä¸ªç¤ºä¾‹çš„å¡«å……é‡ã€‚ å®šä¹‰å¹¶è®­ç»ƒæ¨¡åž‹Three layers:embeddiong layer,RNN,linear layers Embedding layer:transform our sparse one-hot vector into a dense embedding vector.(a single fully connected layer) RNN:take in our dense vector and the previous hidden state ht-1(to calculate the next hidden state ht)RNN(å¾ªçŽ¯ç¥žç»ç½‘ç»œ)ï¼šä¸€ä¸ªåºåˆ—å½“å‰çš„è¾“å‡ºä¸Žå‰é¢çš„è¾“å‡ºæœ‰å…³ Linear layer:take the final hidden state and feed it through a fully connected layer,transforming it to the correct output dimension. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#æ­å»ºæ¨¡åž‹import torch.nn as nnclass RNN(nn.Module): def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim): super().__init__() self.embedding &#x3D; nn.Embedding(input_dim,embedding_dim) self.rnn &#x3D; nn.RNN(embedding_dim,hidden_dim) self.fc &#x3D; nn.Linear(hidden_dim,output_dim) def forward(self,text): #text &#x3D; [sent len,batch size] #embedded &#x3D; [sent len,batch size,emb dim] #output &#x3D; [sent len, batch size,hid dim] #hidden - [1,batch size,hid dim] embedded &#x3D; self.embedding(text) output,hidden &#x3D; self.rnn(embedded) assert torch.equal(output[-1,:,:],hidden.squeeze(0)) return self.fc(hidden.squeeze(0))INPUT_DIM &#x3D; len(TEXT.vocab)EMBEDDING_DIM &#x3D; 100HIDDEN_DIM &#x3D; 256OUTPUT_DIM &#x3D; 1model &#x3D; RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM).to(device)#å®šä¹‰å‡½æ•°æ±‚å¯è®­ç»ƒçš„å‚æ•°æ•°é‡def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f&#39;The model has &#123;count_parameters(model):,&#125; trainable paremeters&#39;)#è®­ç»ƒæ¨¡åž‹import torch.optim as optimoptimizer &#x3D; optim.SGD(model.parameters(),lr&#x3D;1e-3)#ä¼˜åŒ–å™¨ï¼Œæ›´æ–°æ¨¡å—å‚æ•°ï¼›éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ŒSGDcriterion &#x3D; nn.BCEWithLogitsLoss().to(device)#æŸå¤±å‡½æ•°ï¼ŒBCEWithLogitsLossåŒæ—¶æ‰§è¡Œsigmoidå’ŒäºŒå…ƒäº¤å‰ç†µæ­¥éª¤ã€‚def binary_accuracy(preds,y): #å°†é¢„æµ‹ç»“æžœå››èˆäº”å…¥åˆ°æœ€è¿‘ç»“æžœ rounded_preds &#x3D; torch.round(torch.sigmoid(preds)) correct &#x3D; (rounded_preds &#x3D;&#x3D; y).float()#è½¬æ¢ä¸ºæµ®ç‚¹æ•°é™¤æ³• acc &#x3D; correct.sum()&#x2F;len(correct) return accdef train(model,iterator,optimizer,criterion): epoch_loss &#x3D; 0 epoch_acc &#x3D; 0 model.train()#è®©dropoutå’ŒBNç”Ÿæ•ˆï¼Œæ­¤å¤„æ²¡ç”¨åˆ° for batch in iterator: optimizer.zero_grad() predictions &#x3D; model(batch.text).squeeze(1) loss &#x3D; criterion(predictions,batch.label) acc &#x3D; binary_accuracy(predictions,batch.label) loss.backward() #è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ optimizer.step() epoch_loss +&#x3D; loss.item() #.item()ä»Žä¸€ä¸ªåªåŒ…å«ä¸€ä¸ªå€¼çš„å¼ é‡ä¸­æå–æ ‡é‡ã€‚ epoch_acc +&#x3D; acc.item() return epoch_loss&#x2F;len(iterator),epoch_acc&#x2F;len(iterator)def evaluate(model,iterator,criterion): epoch_loss &#x3D; 0 epoch_ass &#x3D; 0 model.eval() with torch.no_grad(): #ä¸è®¡ç®—æ¢¯åº¦ for batch in iterator: predictions &#x3D; model(batch.text).squeeze(1) loss &#x3D; criterion(predictions,batch.label) acc &#x3D; criterion(predictions,batch.label) epoch_loss +&#x3D; loss.item() epoch_ass +&#x3D; acc.item() return epoch_loss&#x2F;len(iterator),epoch_ass&#x2F;len(iterator)import timedef epoch_time(start_time,end_time): elapsed_time &#x3D; end_time-start_time elapsed_mins &#x3D; int(elapsed_time&#x2F;60) elapsed_secs &#x3D; int(elapsed_time-(elapsed_mins*60)) return elapsed_mins,elapsed_secsN_EPOCHS &#x3D; 5best_valid_loss &#x3D; float(&#39;inf&#39;)for epoch in range(N_EPOCHS): start_time &#x3D; time.time() train_loss,train_acc &#x3D; train(model,train_iterator,optimizer,criterion) valid_loss,valid_acc &#x3D; evaluate(model,valid_iterator,criterion) end_time &#x3D; time.time() epoch_mins,epoch_secs &#x3D; epoch_time(start_time,end_time) if valid_loss &lt; best_valid_loss: best_valid_loss &#x3D; valid_loss torch.save(model.state_dict(),&#39;tut1-model.pt&#39;) print(f&#39;Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;&#39;) print(f&#39;\tTrain Loss: &#123;train_loss:.3f&#125; | Train ACC: &#123;train_acc * 100 :.2f&#125;%&#39;) print(f&#39;\tVal Loss: &#123;valid_loss:.3f&#125; | Val ACC: &#123;valid_acc * 100 :.2f&#125;%&#39;)model.load_state_dict(torch.load(&#39;tut1-model.pt&#39;))test_loss,test_acc &#x3D; evaluate(model,test_iterator,criterion)print(f&#39;Test Loss: &#123;test_loss:.3f&#125; | Test Acc: &#123;test_acc *100 :.2f&#125;%&#39;) ç»“æžœ 123456789101112131415161718192021222324252627282930313233Number of training examples: 25000Number of testing examples: 25000Number of training examples: 17500Number of validation examples:7500Number of testing examples: 25000Unique tokens in TEXT vocabulary: 25002Unique tokens in TEXT vocabulary: 2The most common words in the vocabulary and their frequencies:[(&#39;the&#39;, 203566), (&#39;,&#39;, 192495), (&#39;.&#39;, 165618), (&#39;and&#39;, 109442), (&#39;a&#39;, 109116), (&#39;of&#39;, 100702), (&#39;to&#39;, 93766), (&#39;is&#39;, 76328), (&#39;in&#39;, 61255), (&#39;I&#39;, 54004), (&#39;it&#39;, 53508), (&#39;that&#39;, 49187), (&#39;&quot;&#39;, 44282), (&quot;&#39;s&quot;, 43329), (&#39;this&#39;, 42445), (&#39;-&#39;, 36692), (&#39;&#x2F;&gt;&lt;br&#39;, 35752), (&#39;was&#39;, 35034), (&#39;as&#39;, 30384), (&#39;with&#39;, 29774)][&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;the&#39;, &#39;,&#39;, &#39;.&#39;, &#39;and&#39;, &#39;a&#39;, &#39;of&#39;, &#39;to&#39;, &#39;is&#39;]defaultdict(None, &#123;&#39;neg&#39;: 0, &#39;pos&#39;: 1&#125;)The model has 2,592,105 trainable paremetersEpoch: 01 | Epoch Time: 0m 52 Train Loss: 0.694 | Train ACC: 50.28% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 02 | Epoch Time: 0m 54 Train Loss: 0.693 | Train ACC: 49.87% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 03 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.18% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 04 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 49.77% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 05 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.11% Val Loss: 0.697 | Val ACC: 69.71%Test Loss: 0.709 | Test Acc: 70.92% é™„å½•é‡åˆ°çš„ä¸€äº›å‘ï¼šanacondaä¸­å®‰è£…spacyåŽï¼Œå‡ºçŽ°Warning: no model found for â€˜enâ€™ç½‘ä¸Šæœ‰å¾ˆå¤šæ•™ç¨‹å¯ä»¥è§£å†³ï¼Œé€šå¸¸å¯ä»¥ç”¨ä»¥ä¸‹ä»£ç è§£å†³ 1python -m spacy download en_core_web_sm ä½†æ˜¯ä¹Ÿä¼šå­˜åœ¨ä¸€ç›´ä¸‹è½½ä¸ä¸‹æ¥çš„æƒ…å†µï¼Œè¿™æ—¶å¯ä»¥è‡ªå·±ä»Žå®˜ç½‘ä¸Šä¸‹è½½ã€‚æœ€é‡è¦çš„ä¸€ç‚¹å°±æ˜¯ä¸‹è½½æ—¶è¦çœ‹å¥½ç‰ˆæœ¬ï¼Œç½‘ä¸Šæœ‰å¾ˆå¤šæ•™ç¨‹æŽ¨èçš„ç‰ˆæœ¬å·²ç»å¾ˆè€çš„ï¼Œå¦‚æžœä¸‹è½½çš„è¿˜æ˜¯é‚£ä¸ªâ€œè€ç‰ˆæœ¬â€ï¼Œå¯èƒ½å’Œæˆ‘ä¸€æ ·ä¼šå‡ºçŽ°ä»¥ä¸‹æŠ¥é”™re.error: bad escape \p at position 257æ‰€ä»¥ä¸‹è½½æ—¶ä¸€å®šè¦çœ‹å¥½è‡ªå·±spacyçš„ç‰ˆæœ¬ï¼Œç„¶åŽå†åŽ»ä¸‹è½½å¯¹åº”çš„en_core_web_smç‰ˆæœ¬ã€‚æœ€åŽè¾“å…¥ 1pip install en_core_web_sm-2.2.5.tar.gz æ–‡ç« å‚è€ƒ 1 - Simple Sentiment Analysis.ipynb]]></content>
      <tags>
        <tag>Sentiment Analysis</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é‚£äº›å¹´ä¸€èµ·çœ‹è¿‡çš„åŠ¨æ¼«]]></title>
    <url>%2Fyyyeuing.github.io%2F2020%2F02%2F14%2Fcartoon%2F</url>
    <content type="text"><![CDATA[å°é“­åŒå­¦æ˜¯ä¸€ä¸ªèµ„æ·±çš„åŠ¨æ¼«è¿·ï¼Œä¸ºé¿å…å’Œç†¬å¤œå°‘å¥³åƒé¥­çš„æ—¶å€™çŽ©æ‰‹æœºï¼Œå°é“­åŒå­¦å¼€å§‹æŽ¨èåŠ¨æ¼«ã€‚æ­¤æ–‡ç”¨æ¥è®°å½•æŽ¨èé‚£äº›å¹´å°é“­åŒå­¦å’Œç†¬å¤œå°‘å¥³çœ‹è¿‡çš„åŠ¨æ¼«âœŒ ç‹å¦–å°çº¢å¨˜å¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­â­â­ è¿™æ˜¯æˆ‘çœ‹çš„ç¬¬ä¸€éƒ¨åŠ¨æ¼«ï¼Œåˆšå¼€å§‹çœ‹è§‰å¾—æœ‰ç‚¹å¥‡æ€ªï¼Ÿæ„Ÿè§‰æœ‰ç‚¹æ²™é›•ï¼Œä¸æ˜¯ç‰¹åˆ«å–œæ¬¢ï¼ˆä¹‹å‰ä¸€ç›´æ²¡çœ‹è¿‡åŠ¨æ¼«ï¼Œè¯´ä¸æ¸…çš„æ„Ÿè§‰ï¼‰ä½†çœ‹åˆ°åŽé¢è¶Šçœ‹è¶Šä¸Šå¤´ï¼ŒçœŸé¦™çŽ°åœºå“ˆå“ˆå“ˆã€‚ç‹å¦–åˆ†ä¸ºå¾ˆå¤šç¯‡ç« ã€‚æ¯ä¸ªç« èŠ‚ç›¸äº’ç‹¬ç«‹åˆäº’ç›¸å…³è”ï¼ŒçŽ°åœ¨å·²ç»çœ‹å®Œçš„æœ‰ä¸‹æ²™ã€å°¾å£°ã€çŽ‹æƒã€æœˆçº¢ã€åŒ—å±±ã€åƒé¢œã€å—å›½ã€ç«¹ä¸šã€‚é™¤äº†æ„Ÿäººçš„æƒ…æƒ…çˆ±çˆ±ï¼ŒåŠ¨æ¼«é‡Œæž„é€ çš„ä¸–ç•Œä¹Ÿè®©äººå¾ˆç€è¿·ã€‚ç‹å¦–å°¾å£°ç¯‡ä¹Ÿæ¥äº†ï¼å‘œå‘œå‘œï¼Œç–«æƒ…å¾…åœ¨å®¶ä¸èƒ½å’Œå°é“­åŒå­¦ä¸€èµ·çœ‹äº†(ã¥â•¥ï¹â•¥)ã¥ çŽ‹æƒï¼šä¸‡å‰‘ç©¿å¿ƒç»ˆä¸æ‚”ï¼Œç›¸è§†ä¸€ç¬‘è½»çŽ‹æƒã€‚ æœˆçº¢ï¼šå‰ä¸–ä½ å”¤æˆ‘ä¸€å£°å¦–ä»™å§å§ï¼Œä»Šç”Ÿæˆ‘å«ä½ ä¸€å£°é“å£«å“¥å“¥ã€‚ ç«¹ä¸šï¼šä¸ƒæœˆåˆä¸ƒï¼Œæ·®æ°´ç«¹äº­ï¼Œéž˜ç¬›ç›¸ä¾ï¼Œæ— æ€¨æ— æ‚”ã€‚éœ¸ä¸šå’Œæ·®ç«¹æ˜¯æˆ‘çš„æœ€çˆ±ðŸ˜ å·¥ä½œç»†èƒžå¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­â­ çœ‹å®ŒåŽç»å¸¸å°±æ˜¯â€œå¡”å¡”é‚£é…·~â€ï¼Œè¡€å°æ¿çœŸçš„å¥½å¯çˆ±ï¼Œçº¢ç»†èƒžå¾ˆè ¢èŒï¼Œç™½ç»†èƒžå¾ˆåŽ‰å®³ä½†åˆæœ‰ç‚¹å°é—·éªšçš„å¯çˆ±ã€‚é‡Œé¢çš„æ¯ä¸ªç»†èƒžä¸çœ ä¸ä¼‘å„å¸å…¶èŒã€‚é€šè¿‡æ„Ÿå†’ï¼Œè¿‡æ•ï¼Œä¸­æš‘ç­‰ç­‰ç—‡çŠ¶ï¼Œå°†ç»†èƒžçœŸäººåŒ–ï¼Œå±•ç¤ºå‡ºäººä½“é¢å¯¹ç–¾ç—…æ—¶å¾—æ ·è²Œã€‚ï¼ˆè¿™æ®µæ€Žä¹ˆå†™çš„è¿™ä¹ˆå®˜æ–¹å“ˆå“ˆå“ˆï¼‰çœ‹å®Œæ„Ÿè§‰è‡ªå·±æ˜¯çœŸçš„å¥½è…»å®³o(ï¿£â–½ï¿£)ãƒ–ï¼Œå•Šä¸ï¼Œæ˜¯æˆ‘èº«ä½“é‡Œçš„ç»†èƒžå¥½åŽ‰å®³ï¼ä½†æ˜¯æ¯ä¸€é›†åŠ¨æ¼«çš„ä¸»äººå…¬éƒ½ä¼šç”Ÿä¸€æ¬¡ç—…ï¼ŒçœŸæ˜¯è®©äººæ„Ÿæ…¨è¿™ä¸ªå°äººå„¿æœ‰ç‚¹æƒ¨æƒ¨å‘ï¼Œç”Ÿç—…æ¬¡æ•°æœ‰ç‚¹å¤šå‘ï¼Œè€Œä¸”æ®è¯´è¿™ä¸ªäººå„¿æ˜¯ä¸ªå°å­© ç½—å°é»‘æˆ˜è®°å¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­ï¼ˆå‰§å¾ˆå¥½ï¼å°±æ˜¯æ›´æ–°å¤ªæ…¢äº†ï¼ï¼ï¼ï¼‰ çŒ«å¦–å°é»‘ç›—å–å¤©æ˜Žç è¢«è°›å¬å‘çŽ°ï¼Œè¢«æ‰“å›žåŽŸå½¢é‡ä¼¤è€Œé€ƒï¼Œåœ¨æµè½è¡—å¤´ä¹‹æ—¶è¢«ç½—å°ç™½å¸¦å›žå®¶ï¼Œèµ·åç½—å°é»‘ã€‚å®ƒæžé€šäººæ€§ï¼Œä¼šè¹²é©¬æ¡¶ï¼Œä¸åƒçŒ«ç²®ï¼Œé•¿é•¿çš„å°¾å·´å¯ä»¥åˆ†è£‚æˆå¤šä¸ªâ€œé»‘å’»â€ã€‚ã€‚ã€‚ï¼ˆæºè‡ªç™¾åº¦ç™¾ç§‘O(âˆ©_âˆ©)Oï¼‰ç”»é£Žå¾ˆèŒèŒå“’ï¼Œæ¸©é¦¨åˆæœ‰è¶£ã€‚ç”µå½±å’ŒåŠ¨æ¼«éƒ½çœ‹å®Œäº†ï¼Œè¶…çˆ±çš„ä¸€éƒ¨ï¼Œç­‰ä¸åˆ°æ›´æ–°çœŸçš„è®©äººæŠ“ç‹‚ã€‚ è¾‰å¤œå¤§å°å§æƒ³è®©æˆ‘å‘Šç™½å¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­â­ è¿™éƒ¨ç•ªåˆšåˆšå¼€å§‹çœ‹ï¼ŒçœŸçš„æœ‰ç‚¹çœ‹ä¸ä¸‹åŽ»ï¼Œæ„Ÿè§‰æœ‰ç‚¹æ— èŠã€‚ä½†æ˜¯ï¼çœ‹åˆ°åŽé¢ç®€ç›´å°±æ˜¯æ¬²ç½¢ä¸èƒ½å‘€ï¼Œå•Šå•¦å•Šå•¦è±ªå¡å“‡ä¼Šé˜”å¤šè‹ç™½çš„å‰§æƒ…ä»‹ç»ï¼šä¸¤ä¸ªå‚²å¨‡å­¦éœ¸çˆ±æ‹å¯¹æ–¹ï¼Œä¸ºäº†é¢å­å„ç§ç®—è®¡å¯¹æ–¹å¯¹è‡ªå·±è¿›è¡Œå‘Šç™½çš„æ•…äº‹ã€‚æœ‰çš„æ—¶å€™çœ‹çš„æˆ‘çœŸçš„å¾ˆç€æ€¥ï¼Œæƒ³è¦æ‰‹åŠ¨æŒ‰å¤´â•®(â•¯â–½â•°)â•­ï¼ˆé‡Œé¢çš„è—¤åŽŸä¹¦è®°ç»å¯¹æ˜¯ä¸€å¤§äº®ç‚¹ï¼Œè¶…çº§å¯çˆ±ï¼Œä½†ä¹Ÿæœ‰ç‚¹å°è…¹é»‘ï¼Œæˆ‘å¾ˆçˆ±å‘ï¼‰è¾‰å¤œå¤§å°å§ç¬¬äºŒå­£ä¹Ÿæ¥äº†ï¼è¿˜æ˜¯ç–«æƒ…ï¼Œæˆ‘è¿˜æ˜¯ä¸èƒ½å’Œå°é“­åŒå­¦ä¸€èµ·çœ‹(â”¬ï¼¿â”¬)ï¼Œå¸Œæœ›ç–«æƒ…å¿«å¿«ç»“æŸï¼Œå¤§å®¶éƒ½è¦å¥å¥åº·åº·çš„ï¼Œç­‰åˆ°èƒœåˆ©çš„æ—¶å€™ï¼Œå¤§å®¶ä¼™å„¿åˆå¯ä»¥ä¸€èµ·å¼€å¿ƒçš„èšåœ¨ä¸€èµ·å•¦ï¼è¾‰å¤œæˆ‘ä¸€å®šè¦ç•™ç€å’Œå°é“­åŒå­¦ä¸€èµ·çœ‹ï¼ ç´«ç½—å…°æ°¸æ’èŠ±å›­å¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­â­â­ è–‡å°”èŽ‰ç‰¹Â·ä¼ŠèŠ™åŠ ç™»çœŸçš„å¥½ç¾Žï¼Œäººç¾Žï¼Œæ€§æ ¼ç¾Žã€‚å¹¼æ—¶è¢«åŸºå°”ä¼¯ç‰¹å°‘ä½æ¡å›žå®¶ï¼Œä½œä¸ºä¸€ä¸ªå†›äººå¥¹æœ‰éžå‡¡çš„æˆ˜æ–—åŠ›ï¼Œå´ä¸è°™äººäº‹ã€‚ä¸çŸ¥çˆ±ä¸ºä½•ç‰©ï¼Œå¥¹åªæƒ³åšä¸€ä¸ªä¼˜ç§€çš„å†›äººï¼Œåšå°‘ä½å¾—åŠ›çš„å·¦è†€å³è‡‚ã€‚ä¸ºäº†æ˜Žç™½å°‘ä½å£ä¸­çš„â€œçˆ±â€ï¼Œå¥¹åŠªåŠ›çš„æƒ³æˆä¸ºä¸€åå‡ºè‰²çš„â€œè‡ªåŠ¨æ‰‹è®°äººå¶â€ï¼Œåœ¨ä¸€æ¬¡æ¬¡çš„ä»£å†™ä¹¦ä¿¡çš„è¿‡ç¨‹ä¸­ï¼Œä»Žæ²¡æœ‰å¿ƒåˆ°æ‹¥æœ‰ä¸€é¢—æŸ”è½¯çš„å¿ƒã€‚é‡Œé¢æœ‰å¾ˆå¤šæ„Ÿäººçš„æ•…äº‹ï¼Œéžå¸¸é€‚åˆä¸€ä¸ªäººåœ¨å®‰é™çš„çŽ¯å¢ƒä¸‹çœ‹ï¼Œå¾ˆæ²»æ„ˆã€‚æœ€è¿‘çœ‹Bç«™ï¼Œæœ‰å¼¹å¹•è¯´å°‘ä½æ²¡æœ‰æ­»ï¼ç»“å±€æ˜¯å¥½çš„ï¼æˆ‘å¤ªå¼€å¿ƒäº†ï¼ï¼ˆè™½ç„¶ä¸çŸ¥é“æ˜¯ä¸æ˜¯çœŸçš„ï¼Œå°±ç®—æ˜¯å‡çš„ï¼Œæˆ‘ä¹Ÿè¦è„‘è¡¥æ˜¯çœŸçš„ã€‚ï¼‰ é¬¼ç­ä¹‹åˆƒå¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­â­â­ ä¸€éƒ¨çƒ­è¡€åŠ¨æ¼«ï¼Œç”»é£Žå¾ˆå–œæ¬¢ï¼Œé¢˜æä¹Ÿå¾ˆå–œæ¬¢ã€‚ç‚­æ²»éƒŽçš„å®¶äººè¢«æ¶é¬¼åƒäº†ï¼Œæ¯äº²å’Œå››ä¸ªå¼Ÿå¼Ÿå¦¹å¦¹éƒ½è¢«æ€æ­»ï¼Œå”¯ä¸€ç”Ÿè¿˜çš„å¦¹å¦¹ç¥¢è±†å­ä¹Ÿå˜æˆäº†é¬¼ã€‚ä¸ºäº†è®©å¦¹å¦¹ç¥¢è±†å­å˜å›žäººç±»ï¼Œä¸ºäº†è®¨ä¼æ€å®³å®¶äººçš„æ¶é¬¼ï¼Œç‚­æ²»éƒŽæˆä¸ºäº†â€œæ€é¬¼é˜Ÿâ€çš„ä¸€å‘˜ã€‚æ€»çš„æ¥è¯´å°±æ˜¯çƒ­è¡€å°‘å¹´ä¸ºæ•‘å¦¹è¸ä¸Šç­é¬¼ä¹‹è·¯çš„æ•…äº‹ã€‚è¶…çº§è¶…çº§å–œæ¬¢ï¼ï¼ï¼æœ€è¿‘å‡ æœŸçš„åŠ¨æ¼«æ›´æ–°å¥½åƒåè¿‡å±±è½¦ï¼Œç‚­æ²»éƒŽå˜é¬¼äº†ï¼Ÿç‚­æ²»éƒŽåˆä¸æ˜¯é¬¼äº†ï¼Ÿé ç€å¾®åšçƒ­æœçœ‹å®Œäº†å¤§ç»“å±€ï¼Œæ„æ–™ä¹‹å¤–æƒ…ç†ä¹‹ä¸­ç½¢äº†ã€‚æœŸå¾…å‰§ç‰ˆã€‚ åˆºå®¢äº”å…­ä¸ƒå¿ƒçˆ±æŒ‡æ•°ï¼šâ­â­â­â­â­ é˜¿ççˆ±ä¸Šäº†é˜¿å¼ºã€‚ ä»Šå¤©æˆ‘å°±è¦å¸¦å¥¹èµ°ï¼Œæˆ‘çœ‹è°æ•¢æ‹¦æˆ‘ï¼ ä½ ä¸è¯´å‡ºæ¥ å¯¹æ–¹å°±æ°¸è¿œä¸çŸ¥é“ä½ çš„å¿ƒæ„ æ¯ä¸€ä¸ªå¼•å¾—è§‚ä¼—å‘ç¬‘çš„äººç‰©ï¼Œå…¶å®žéƒ½æœ‰ä¸€ä¸ªæ‚²æƒ…çš„å†…æ ¸ã€‚ ä¸æ‹¼å°½å…¨åŠ›åŽ»è¯•ä¸€ä¸‹ï¼Œåˆæ€Žä¹ˆä¼šçŸ¥é“å•Š æˆ‘æ˜¯åˆºå®¢äº”å…­ä¸ƒï¼Œåˆºå®¢æŽ’è¡Œæ¦œä¸€ä¸‡ä¸ƒåƒä¸‰ç™¾å…­åä¹ä½ï¼Œæœ€æ“…é•¿å‰ªç©ºæ°”åˆ˜æµ·ï¼Œä¸€ç›´ä»¥ä¼˜è´¨çš„æœåŠ¡å’Œäº²æ°‘çš„ä»·æ ¼æ·±å—æ‘æ°‘çš„å–œçˆ±ï¼Œç›®å‰çš„æƒ…æ„ŸçŠ¶å†µæ˜¯å•èº«ï¼Œä¸å¦‚ä¸€èµ·åŽ»å–æ¯ä¸œè¥¿äº¤æµ..æ˜Žæ˜Žæ˜¯ä¸€éƒ¨æžç¬‘çš„ç•ªï¼Œå´åˆæœ‰å¾ˆå¤šè®©äººæ„ŸåŠ¨çš„åœ°æ–¹ã€‚]]></content>
      <tags>
        <tag>åŠ¨æ¼«</tag>
        <tag>ä¼‘é—²</tag>
      </tags>
  </entry>
</search>
