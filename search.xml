<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2.Transformers for text classification]]></title>
    <url>%2Fyyyeuing.github.io%2F2020%2F06%2F27%2Fbert-clue%2F</url>
    <content type="text"><![CDATA[基于bert实现的文本分类 数据处理 数据集使用TNEWS下载地址 数据集包括：训练集(53,360)，验证集(10,000)，测试集(10,000) 每条数据包括三个属性：分类的ID，分类的名称，新闻字符串。其中共包含15个标签。 eg: {“label”: “108”, “label_desc”: “news_edu”, “sentence”: “上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？”, “keywords”: “”} 定义InputExample类，其中包括：guid，text_a,text_b,label着四个属性。由于是单行文本分类的问题，text_b用不到，定义为None。 1234567# 读取json文件class InputExample(object): def __init__(self,guid,text_a,text_b&#x3D;None,lable &#x3D; None): self.guid &#x3D; guid self.text_a &#x3D; text_a self.text_b &#x3D; text_b self.label &#x3D; lable 按行读取数据，每行读取到的[sentence]部分即为分类的语句，将其转换成一个一个的token后，再将token转换成对应的id。最后返回的是一个列表examples，里面包含每条处理后的数据。 1234567891011121314151617181920212223# 读取json文件def read_json(input_file): with open(input_file, &#39;r&#39;, encoding&#x3D;&#39;UTF-8&#39;) as f: reader &#x3D; f.readlines() lines &#x3D; [] for line in reader: lines.append(json.loads(line.strip())) # print(lines) return linesdef get_examples(lines,set_type): examples &#x3D; [] for (i, line) in enumerate (lines): guid &#x3D; &quot;%s-%s&quot; % (set_type, i) text_a &#x3D; tokenizer.convert_tokens_to_ids(tokenizer.tokenize(lines[i][&#39;sentence&#39;])) text_b &#x3D; None label &#x3D; str(line[&#39;label&#39;]) if set_type !&#x3D; &#39;test&#39; else &quot;100&quot; examples.append(InputExample(guid &#x3D; guid,text_a&#x3D;text_a,text_b&#x3D;text_b, lable&#x3D; label)) # print(len(examples)) return examplesdef get_any_examples(data_dir): return get_examples(read_json(data_dir), os.path.splitext(data_dir)[0]) 网络模型使用bert预训练模型，具体可参考huggingface官方文档 . bert模型类的输入主要分以下四个部分： input_ids:文本对应的id序列,shape=[batch_size, sequence_length] attention_mask:文本对应的pad标记序列,1表示该位置未被填充，0表示被填充。shape=[batch_size, sequence_length], token_type_ids:用于区分文本中两个句子的标记序列,0表示相应token属于第一个句子，1表示相应token属于第二个句子。shape=[batch_size, sequence_length] 代码实现：在bert中，限定输入的长度不能超过512，故定义max_length=512。不及512的地方补0，超过的进行裁剪。定义四个列表：id_list、attention_masks_list、token_type_ids_list、labels_list，将每条数据转换得来的输入存入到相应的列表，最后将列表存入data字典中。 12345678910111213141516171819202122232425262728293031323334353637383940414243def preprocess(data_dir, max_length &#x3D; 512): &quot;&quot;&quot; 处理数据 :param raw_data_fn: 原始数据文件名 :return: &quot;&quot;&quot; examples &#x3D; get_any_examples(data_dir) id_list &#x3D; [] attention_masks_list &#x3D; [] token_type_ids_list &#x3D; [] labels_list &#x3D; [] for example in examples: input_ids &#x3D; example.text_a label_list &#x3D; get_lables() label_map &#x3D; &#123;label: i for i ,label in enumerate(label_list)&#125; label &#x3D; label_map[example.label] if len(input_ids) &gt; max_length: token &#x3D; tokenizer.tokenize(input_ids) input_ids &#x3D; token[:max_length-2] input_len &#x3D; len(input_ids) padding_length &#x3D; max_length-input_len attention_masks &#x3D; [1] * input_len + [0] * padding_length input_ids &#x3D; input_ids + ([0] * padding_length) token_type_ids &#x3D;[0] * max_length id_list.append(input_ids) attention_masks_list.append(attention_masks) token_type_ids_list.append(token_type_ids) labels_list.append(label) data &#x3D; &#123;&#39;input_ids&#39;:id_list, &#39;attention_masks&#39;:attention_masks_list, &#39;token_type_ids&#39;:token_type_ids_list, &#39;input_labels&#39;:labels_list&#125; return data bert模型调用该任务用到的是transformers提供的BertForSequenceClassification1234# 加载模型及配置方法 bert_config &#x3D; BertConfig.from_pretrained(model_name, num_labels&#x3D;15) # 头条文本分类数据集为15类 model &#x3D; BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path&#x3D;model_name, config&#x3D;bert_config, cache_dir&#x3D;cache_dir) optimizer &#x3D; torch.optim.Adam(model.parameters(), lr&#x3D;lr) 模型训练该任务是单行文本问题，只需要输入input_ids和labels即可。output[0]为loss。(查看BertForSequenceClassification类函数可更清楚的了解模型的输入和输出) 123456789101112131415161718192021222324252627282930313233def train(model, optimizer, data_loader): min_loss &#x3D; float(&#39;inf&#39;) for e in range(epochs): for step,batch in enumerate(data_loader): model.zero_grad() model.train() inputs &#x3D; &#123;&#39;input_ids&#39;:batch[0], &#39;attention_mask&#39;:batch[1], &#39;token_tpye_ids&#39;:batch[2], &#39;labels&#39;:batch[3] &#125; output &#x3D; model(input_ids &#x3D; inputs[&#39;input_ids&#39;], labels &#x3D; inputs[&#39;labels&#39;]) # print(output[0]) train_loss &#x3D; output[0] train_loss &#x3D; train_loss &#x2F; accumulation_steps train_loss.backward() # 每八次更新一下网络中的参数 if (step+1) % accumulation_steps &#x3D;&#x3D; 0: optimizer.step() optimizer.zero_grad() if (step+1) % accumulation_steps &#x3D;&#x3D; 1: print(&#39;Train Epoch: &#123;&#125; [&#123;&#125;&#x2F;&#123;&#125;] || train_loss: &#123;:.6f&#125;&#39;.format( e+1, step * batch_size, len(data_loader.dataset), train_loss.item() )) print(&#39;Train Epoch: &#123;&#125; || train_loss:&#123;:.6f&#125;.&#39;.format(e+1,train_loss.item())) if train_loss &lt; min_loss: min_loss &#x3D; train_loss torch.save(model.state_dict(), path) 附： bert超详细讲解播客(๑•̀ㅂ•́)و✧CLUE benchmark代码]]></content>
      <tags>
        <tag>nlp</tag>
        <tag>文本分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1-Simple Sentiment Analysis]]></title>
    <url>%2Fyyyeuing.github.io%2F2020%2F04%2F30%2FTextClassifier%2F</url>
    <content type="text"><![CDATA[电影评论情感分类最简版 Simple Sentiment Analysis数据处理 下载数据集，切分成训练集，验证集，测试集。 建立词汇表。由于词汇过多会导致训练时间过长，需要进行简化，可以通过选取最常出现的n个词汇，或者舍弃出现次数少于m次的词汇。每次输入是一批句子，句子太短的用pad进行填充，其次用unk来填充被舍弃的词汇。 创建迭代器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchfrom torchtext import dataimport spacy#随机种子繁衍SEED &#x3D; 1234torch.manual_seed(SEED)torch.backends.cudnn.deterministic &#x3D; TrueTEXT &#x3D; data.Field(tokenize&#x3D;&#39;spacy&#39;,tokenizer_language&#x3D;&#39;en_core_web_sm&#39;)LABEL &#x3D; data.LabelField(dtype&#x3D;torch.float)#下载数据集切分成训练集和测试集from torchtext import datasetstrain_data,test_data &#x3D; datasets.IMDB.splits(TEXT,LABEL)print(f&#39;Number of training examples: &#123;len(train_data)&#125;&#39;)print(f&#39;Number of testing examples: &#123;len(test_data)&#125;&#39;)#print(vars(train_data.examples[0]))#将训练集切分为训练集和验证集import randomtrain_data,valid_data &#x3D; train_data.split(random_state &#x3D; random.seed(SEED))print(f&#39;Number of training examples: &#123;len(train_data)&#125;&#39;)print(f&#39;Number of validation examples:&#123;len(valid_data)&#125;&#39;)print(f&#39;Number of testing examples: &#123;len(test_data)&#125;&#39;)#建立词汇表MAX_VOCAB_SIZE &#x3D; 25000TEXT.build_vocab(train_data,max_size &#x3D; MAX_VOCAB_SIZE)LABEL.build_vocab(train_data)#输出“Unique tokens in TEXT vocabulary: 25002”是由于添加了&lt;unk&gt;和&lt;pad&gt;#&lt;unk&gt;是补充丢弃的词汇#&lt;pad&gt;是由于每次是输入一批句子，句子的长度要求一样，短句子会被填充print(f&#39;Unique tokens in TEXT vocabulary: &#123;len(TEXT.vocab)&#125;&#39;)print(f&#39;Unique tokens in TEXT vocabulary: &#123;len(LABEL.vocab)&#125;&#39;)print(f&#39;The most common words in the vocabulary and their frequencies:&#123;TEXT.vocab.freqs.most_common(20)&#125;&#39;)print(TEXT.vocab.itos[:10])# stoi (string to int) , itos (int to string)print(LABEL.vocab.stoi) #0:negative,1:positive###迭代BATCH_SIZE &#x3D; 64device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)train_iterator,valid_iterator,test_iterator &#x3D; data.BucketIterator.splits( (train_data,valid_data,test_data), batch_size&#x3D;BATCH_SIZE, device &#x3D; device)#返回一批示例，其中每个示例的长度都相似，从而最小化每个示例的填充量。 定义并训练模型Three layers:embeddiong layer,RNN,linear layers Embedding layer:transform our sparse one-hot vector into a dense embedding vector.(a single fully connected layer) RNN:take in our dense vector and the previous hidden state ht-1(to calculate the next hidden state ht)RNN(循环神经网络)：一个序列当前的输出与前面的输出有关 Linear layer:take the final hidden state and feed it through a fully connected layer,transforming it to the correct output dimension. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#搭建模型import torch.nn as nnclass RNN(nn.Module): def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim): super().__init__() self.embedding &#x3D; nn.Embedding(input_dim,embedding_dim) self.rnn &#x3D; nn.RNN(embedding_dim,hidden_dim) self.fc &#x3D; nn.Linear(hidden_dim,output_dim) def forward(self,text): #text &#x3D; [sent len,batch size] #embedded &#x3D; [sent len,batch size,emb dim] #output &#x3D; [sent len, batch size,hid dim] #hidden - [1,batch size,hid dim] embedded &#x3D; self.embedding(text) output,hidden &#x3D; self.rnn(embedded) assert torch.equal(output[-1,:,:],hidden.squeeze(0)) return self.fc(hidden.squeeze(0))INPUT_DIM &#x3D; len(TEXT.vocab)EMBEDDING_DIM &#x3D; 100HIDDEN_DIM &#x3D; 256OUTPUT_DIM &#x3D; 1model &#x3D; RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM).to(device)#定义函数求可训练的参数数量def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)print(f&#39;The model has &#123;count_parameters(model):,&#125; trainable paremeters&#39;)#训练模型import torch.optim as optimoptimizer &#x3D; optim.SGD(model.parameters(),lr&#x3D;1e-3)#优化器，更新模块参数；随机梯度下降法，SGDcriterion &#x3D; nn.BCEWithLogitsLoss().to(device)#损失函数，BCEWithLogitsLoss同时执行sigmoid和二元交叉熵步骤。def binary_accuracy(preds,y): #将预测结果四舍五入到最近结果 rounded_preds &#x3D; torch.round(torch.sigmoid(preds)) correct &#x3D; (rounded_preds &#x3D;&#x3D; y).float()#转换为浮点数除法 acc &#x3D; correct.sum()&#x2F;len(correct) return accdef train(model,iterator,optimizer,criterion): epoch_loss &#x3D; 0 epoch_acc &#x3D; 0 model.train()#让dropout和BN生效，此处没用到 for batch in iterator: optimizer.zero_grad() predictions &#x3D; model(batch.text).squeeze(1) loss &#x3D; criterion(predictions,batch.label) acc &#x3D; binary_accuracy(predictions,batch.label) loss.backward() #计算每个参数的梯度 optimizer.step() epoch_loss +&#x3D; loss.item() #.item()从一个只包含一个值的张量中提取标量。 epoch_acc +&#x3D; acc.item() return epoch_loss&#x2F;len(iterator),epoch_acc&#x2F;len(iterator)def evaluate(model,iterator,criterion): epoch_loss &#x3D; 0 epoch_ass &#x3D; 0 model.eval() with torch.no_grad(): #不计算梯度 for batch in iterator: predictions &#x3D; model(batch.text).squeeze(1) loss &#x3D; criterion(predictions,batch.label) acc &#x3D; criterion(predictions,batch.label) epoch_loss +&#x3D; loss.item() epoch_ass +&#x3D; acc.item() return epoch_loss&#x2F;len(iterator),epoch_ass&#x2F;len(iterator)import timedef epoch_time(start_time,end_time): elapsed_time &#x3D; end_time-start_time elapsed_mins &#x3D; int(elapsed_time&#x2F;60) elapsed_secs &#x3D; int(elapsed_time-(elapsed_mins*60)) return elapsed_mins,elapsed_secsN_EPOCHS &#x3D; 5best_valid_loss &#x3D; float(&#39;inf&#39;)for epoch in range(N_EPOCHS): start_time &#x3D; time.time() train_loss,train_acc &#x3D; train(model,train_iterator,optimizer,criterion) valid_loss,valid_acc &#x3D; evaluate(model,valid_iterator,criterion) end_time &#x3D; time.time() epoch_mins,epoch_secs &#x3D; epoch_time(start_time,end_time) if valid_loss &lt; best_valid_loss: best_valid_loss &#x3D; valid_loss torch.save(model.state_dict(),&#39;tut1-model.pt&#39;) print(f&#39;Epoch: &#123;epoch+1:02&#125; | Epoch Time: &#123;epoch_mins&#125;m &#123;epoch_secs&#125;&#39;) print(f&#39;\tTrain Loss: &#123;train_loss:.3f&#125; | Train ACC: &#123;train_acc * 100 :.2f&#125;%&#39;) print(f&#39;\tVal Loss: &#123;valid_loss:.3f&#125; | Val ACC: &#123;valid_acc * 100 :.2f&#125;%&#39;)model.load_state_dict(torch.load(&#39;tut1-model.pt&#39;))test_loss,test_acc &#x3D; evaluate(model,test_iterator,criterion)print(f&#39;Test Loss: &#123;test_loss:.3f&#125; | Test Acc: &#123;test_acc *100 :.2f&#125;%&#39;) 结果 123456789101112131415161718192021222324252627282930313233Number of training examples: 25000Number of testing examples: 25000Number of training examples: 17500Number of validation examples:7500Number of testing examples: 25000Unique tokens in TEXT vocabulary: 25002Unique tokens in TEXT vocabulary: 2The most common words in the vocabulary and their frequencies:[(&#39;the&#39;, 203566), (&#39;,&#39;, 192495), (&#39;.&#39;, 165618), (&#39;and&#39;, 109442), (&#39;a&#39;, 109116), (&#39;of&#39;, 100702), (&#39;to&#39;, 93766), (&#39;is&#39;, 76328), (&#39;in&#39;, 61255), (&#39;I&#39;, 54004), (&#39;it&#39;, 53508), (&#39;that&#39;, 49187), (&#39;&quot;&#39;, 44282), (&quot;&#39;s&quot;, 43329), (&#39;this&#39;, 42445), (&#39;-&#39;, 36692), (&#39;&#x2F;&gt;&lt;br&#39;, 35752), (&#39;was&#39;, 35034), (&#39;as&#39;, 30384), (&#39;with&#39;, 29774)][&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;the&#39;, &#39;,&#39;, &#39;.&#39;, &#39;and&#39;, &#39;a&#39;, &#39;of&#39;, &#39;to&#39;, &#39;is&#39;]defaultdict(None, &#123;&#39;neg&#39;: 0, &#39;pos&#39;: 1&#125;)The model has 2,592,105 trainable paremetersEpoch: 01 | Epoch Time: 0m 52 Train Loss: 0.694 | Train ACC: 50.28% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 02 | Epoch Time: 0m 54 Train Loss: 0.693 | Train ACC: 49.87% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 03 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.18% Val Loss: 0.697 | Val ACC: 69.70%Epoch: 04 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 49.77% Val Loss: 0.697 | Val ACC: 69.69%Epoch: 05 | Epoch Time: 0m 53 Train Loss: 0.693 | Train ACC: 50.11% Val Loss: 0.697 | Val ACC: 69.71%Test Loss: 0.709 | Test Acc: 70.92% 附录遇到的一些坑：anaconda中安装spacy后，出现Warning: no model found for ‘en’网上有很多教程可以解决，通常可以用以下代码解决 1python -m spacy download en_core_web_sm 但是也会存在一直下载不下来的情况，这时可以自己从官网上下载。最重要的一点就是下载时要看好版本，网上有很多教程推荐的版本已经很老的，如果下载的还是那个“老版本”，可能和我一样会出现以下报错re.error: bad escape \p at position 257所以下载时一定要看好自己spacy的版本，然后再去下载对应的en_core_web_sm版本。最后输入 1pip install en_core_web_sm-2.2.5.tar.gz 文章参考 1 - Simple Sentiment Analysis.ipynb]]></content>
      <tags>
        <tag>Sentiment Analysis</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年一起看过的动漫]]></title>
    <url>%2Fyyyeuing.github.io%2F2020%2F02%2F14%2Fcartoon%2F</url>
    <content type="text"><![CDATA[小铭同学是一个资深的动漫迷，为避免和熬夜少女吃饭的时候玩手机，小铭同学开始推荐动漫。此文用来记录推荐那些年小铭同学和熬夜少女看过的动漫✌ 狐妖小红娘心爱指数：⭐⭐⭐⭐⭐ 这是我看的第一部动漫，刚开始看觉得有点奇怪？感觉有点沙雕，不是特别喜欢（之前一直没看过动漫，说不清的感觉）但看到后面越看越上头，真香现场哈哈哈。狐妖分为很多篇章。每个章节相互独立又互相关联，现在已经看完的有下沙、尾声、王权、月红、北山、千颜、南国、竹业。除了感人的情情爱爱，动漫里构造的世界也让人很着迷。狐妖尾声篇也来了！呜呜呜，疫情待在家不能和小铭同学一起看了(づ╥﹏╥)づ 王权：万剑穿心终不悔，相视一笑轻王权。 月红：前世你唤我一声妖仙姐姐，今生我叫你一声道士哥哥。 竹业：七月初七，淮水竹亭，鞘笛相依，无怨无悔。霸业和淮竹是我的最爱😍 工作细胞心爱指数：⭐⭐⭐⭐ 看完后经常就是“塔塔那酷~”，血小板真的好可爱，红细胞很蠢萌，白细胞很厉害但又有点小闷骚的可爱。里面的每个细胞不眠不休各司其职。通过感冒，过敏，中暑等等症状，将细胞真人化，展示出人体面对疾病时得样貌。（这段怎么写的这么官方哈哈哈）看完感觉自己是真的好腻害o(￣▽￣)ブ，啊不，是我身体里的细胞好厉害！但是每一集动漫的主人公都会生一次病，真是让人感慨这个小人儿有点惨惨呐，生病次数有点多呐，而且据说这个人儿是个小孩 罗小黑战记心爱指数：⭐⭐⭐（剧很好！就是更新太慢了！！！） 猫妖小黑盗取天明珠被谛听发现，被打回原形重伤而逃，在流落街头之时被罗小白带回家，起名罗小黑。它极通人性，会蹲马桶，不吃猫粮，长长的尾巴可以分裂成多个“黑咻”。。。（源自百度百科O(∩_∩)O）画风很萌萌哒，温馨又有趣。电影和动漫都看完了，超爱的一部，等不到更新真的让人抓狂。 辉夜大小姐想让我告白心爱指数：⭐⭐⭐⭐ 这部番刚刚开始看，真的有点看不下去，感觉有点无聊。但是！看到后面简直就是欲罢不能呀，啊啦啊啦豪卡哇伊阔多苍白的剧情介绍：两个傲娇学霸爱恋对方，为了面子各种算计对方对自己进行告白的故事。有的时候看的我真的很着急，想要手动按头╮(╯▽╰)╭（里面的藤原书记绝对是一大亮点，超级可爱，但也有点小腹黑，我很爱呐）辉夜大小姐第二季也来了！还是疫情，我还是不能和小铭同学一起看(┬＿┬)，希望疫情快快结束，大家都要健健康康的，等到胜利的时候，大家伙儿又可以一起开心的聚在一起啦！辉夜我一定要留着和小铭同学一起看！ 紫罗兰永恒花园心爱指数：⭐⭐⭐⭐⭐ 薇尔莉特·伊芙加登真的好美，人美，性格美。幼时被基尔伯特少佐捡回家，作为一个军人她有非凡的战斗力，却不谙人事。不知爱为何物，她只想做一个优秀的军人，做少佐得力的左膀右臂。为了明白少佐口中的“爱”，她努力的想成为一名出色的“自动手记人偶”，在一次次的代写书信的过程中，从没有心到拥有一颗柔软的心。里面有很多感人的故事，非常适合一个人在安静的环境下看，很治愈。最近看B站，有弹幕说少佐没有死！结局是好的！我太开心了！（虽然不知道是不是真的，就算是假的，我也要脑补是真的。） 鬼灭之刃心爱指数：⭐⭐⭐⭐⭐ 一部热血动漫，画风很喜欢，题材也很喜欢。炭治郎的家人被恶鬼吃了，母亲和四个弟弟妹妹都被杀死，唯一生还的妹妹祢豆子也变成了鬼。为了让妹妹祢豆子变回人类，为了讨伐杀害家人的恶鬼，炭治郎成为了“杀鬼队”的一员。总的来说就是热血少年为救妹踏上灭鬼之路的故事。超级超级喜欢！！！最近几期的动漫更新好像坐过山车，炭治郎变鬼了？炭治郎又不是鬼了？靠着微博热搜看完了大结局，意料之外情理之中罢了。期待剧版。 刺客五六七心爱指数：⭐⭐⭐⭐⭐ 阿珍爱上了阿强。 今天我就要带她走，我看谁敢拦我！ 你不说出来 对方就永远不知道你的心意 每一个引得观众发笑的人物，其实都有一个悲情的内核。 不拼尽全力去试一下，又怎么会知道啊 我是刺客五六七，刺客排行榜一万七千三百六十九位，最擅长剪空气刘海，一直以优质的服务和亲民的价格深受村民的喜爱，目前的情感状况是单身，不如一起去喝杯东西交流..明明是一部搞笑的番，却又有很多让人感动的地方。]]></content>
      <tags>
        <tag>动漫</tag>
        <tag>休闲</tag>
      </tags>
  </entry>
</search>
